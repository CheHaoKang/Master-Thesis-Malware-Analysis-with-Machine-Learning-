import datetime
import json
import os
import sys
from sklearn import svm
from sklearn import cluster
import numpy as np
from os import listdir
import getpass
from os.path import isdir
from sklearn import preprocessing
#import xgboost as xgb
from sklearn import decomposition
from sklearn import datasets
import pylab as pl
from sklearn.feature_extraction.text import TfidfTransformer
import scipy.sparse as sp
import distance # Compute String distances
from sklearn.feature_extraction.text import CountVectorizer
from os.path import isfile, join
from random import randint
import requests
import json
import subprocess
import time
from operator import itemgetter, attrgetter, methodcaller
import pickle
import copy
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import SelectFpr
from sklearn.feature_selection import SelectFdr
from sklearn.feature_selection import SelectFwe
import numpy
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import RFE
from collections import Counter
from sklearn.decomposition import NMF
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics.cluster import v_measure_score
from sklearn.metrics.cluster import homogeneity_completeness_v_measure
from sklearn.metrics import make_scorer
from sklearn.model_selection import StratifiedKFold
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph
from sklearn.externals.joblib import Memory
import pandas as pd
from random import shuffle
#from subprocess import Popen, PIPE, STDOUT
from shutil import copyfile
import collections

def removeNt_Ex_W_A(api):
    if api[-1]=='W' or api[-1]=='A':
        api = api[:-1]

    if api[-2] == 'E' and api[-1] == 'x':
        api = api[:-2]

    if api[0] == 'N' and api[1] == 't':
        api = api[2:]

    return api


def createApiVector(familyDirectoryDict):
    # This vector is used to store information of report.jsons
    apiVector = list()

    folderCounter = 0
    path = "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/storage/analyses/"
    for family, ids in sorted(familyDirectoryDict.items()):
        print("\nFamily + ids ==>" + str(family) + str(ids))
        for id in ids:
            apiSummary = dict()

            file = open(path + str(id) + "/reports/report.json", 'r')  # , encoding='UTF-8')
            report = file.read()
            jsonInfo = json.loads(report)

            print("\n" + path + str(id) + "/reports/report.json")

            #>>>test
            originalNameProcessID = -1
            for processInfo in jsonInfo['behavior']['generic']:
                # print(">>>" + str(processInfo['process_name']) + "<<<")
                # print(">>>" + str(processInfo['pid']) + "<<<")

                if "OriginalName_" in processInfo['process_name']:
                    originalNameProcessID = processInfo['pid']
                    break
            print("OriginalNameProcessID >>>" + str(originalNameProcessID))
            #<<<test

            for pid in jsonInfo['behavior']['apistats'].keys():
                # if int(pid) != int(originalNameProcessID):
                #     print("pid != originalNameProcessID => " + str(pid) + " " + str(originalNameProcessID))
                #     continue
                #print(jsonInfo['behavior']['apistats'][pid])

                for api in jsonInfo['behavior']['apistats'][pid].keys():
                    backUpApi = copy.deepcopy(api)
                    apiRemoved = removeNt_Ex_W_A(api)

                    if apiRemoved in apiSummary:
                        apiSummary[apiRemoved] += jsonInfo['behavior']['apistats'][pid][backUpApi]
                    else:
                        apiSummary[apiRemoved] = jsonInfo['behavior']['apistats'][pid][backUpApi]

            apiListFromFile = list()
            if os.path.exists('apiList.txt'):
                apiListFile = open('apiList.txt', 'r')
                for api in apiListFile:
                    apiListFromFile.append(api.rstrip())
                apiListFile.close()

            apiListFile = open('apiList.txt', 'a')
            for api in sorted(apiSummary):
                if api not in apiListFromFile:
                    apiListFile.write(api + "\n")
                    apiListFromFile.append(api)
            apiListFile.close()

            apiVector.append([])
            apiListFromFileDict = dict()
            for i in xrange(len(apiListFromFile)):
                apiVector[folderCounter].append(0)
                apiListFromFileDict[apiListFromFile[i]] = i

            for api in apiSummary.keys():
                apiVector[folderCounter][apiListFromFileDict[api]] = apiSummary[api]
                # print(str(folderCounter), str(apiListFromFileDict[api]))
                # print(str(api) + " => " + str(apiVector[folderCounter][apiListFromFileDict[api]]))

            folderCounter += 1
            file.close()

    # Go through apiVector to append 0 to members which don't have certain api functions of the last member
    for i in xrange(len(apiVector)-1):
        print("\ni: " + str(i))
        #
        # print("Origin - " + str(len(apiVector[i])))
        # print(str(apiVector[i]))

        if len(apiVector[i]) < len(apiVector[folderCounter-1]):
            for j in xrange(len(apiVector[folderCounter-1]) - len(apiVector[i])):
                apiVector[i].append(0)

        print(str(apiVector[i]) + "\n")


    return apiVector


# behavior -> processes -> process_path + calls ->
def createApiSequenceVector(familyDirectoryDict):
    # This vector is used to store information of report.jsons
    apiSequenceVector = list()

    # Get the api list from 'apiList.txt'
    apiCounter = 0
    apiListFromFileDict = dict()

    if os.path.exists('apiList.txt'):
        apiListFile = open('apiList.txt', 'r')
        for api in apiListFile:
            apiListFromFileDict[api.rstrip()] = apiCounter
            apiCounter += 1
        apiListFile.close()
    print("apiListFromFileDict =>", apiListFromFileDict)

    folderCounter = 0
    path = "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/storage/analyses/"
    for family, ids in sorted(familyDirectoryDict.items()):
        print("\nFamily + ids ==>" + str(family) + str(ids))
        for id in ids:
            file = open(path + str(id) + "/reports/report.json", 'r')  # , encoding='UTF-8')
            report = file.read()
            jsonInfo = json.loads(report)

            print("\n" + path + str(id) + "/reports/report.json")

            # append a new list for saving an apisequence.
            apiSequenceVector.append([])

            originalProcess=False
            for process in jsonInfo['behavior']['processes']:
                # if "OriginalName_" not in process['process_path']:
                #     continue
                # else:
                #     originalProcess = True
                print(process['process_path'])

                first = 2
                for callInfo in process['calls']:
                    if first!=0:
                        print(callInfo['api'])
                        first -= 1

                    apiCall =  callInfo['api']
                    apiCallRemoved = removeNt_Ex_W_A(apiCall)
                    apiSequenceVector[folderCounter].append(apiListFromFileDict[apiCallRemoved])

            # if originalProcess==False:
            #     print("Cannot find a process with the same name!")
            print(apiSequenceVector[folderCounter])

            folderCounter += 1
            file.close()  # file = open(folder + "/reports/report.json", 'r')

    return apiSequenceVector
            #print(process)
            # for api in jsonInfo['behavior']['processes'][process].keys():
            #     # print(jsonInfo['behavior']['apistats'][pid][api])
            #     apiRemoved = removeEx_W_A(api)
            #
            #     if apiRemoved in apiSummary:
            #         apiSummary[apiRemoved] += jsonInfo['behavior']['processes'][process][api]
            #     else:
            #         apiSummary[apiRemoved] = jsonInfo['behavior']['processes'][process][api]

def createApiSequenceVectorByTime(familyDirectoryDict):
    # This vector is used to store information of report.jsons
    apiSequenceVector = list()

    # Get the api list from 'apiList.txt'
    apiCounter = 0
    apiListFromFileDict = dict()

    if os.path.exists('apiList.txt'):
        apiListFile = open('apiList.txt', 'r')
        for api in apiListFile:
            apiListFromFileDict[api.rstrip()] = apiCounter
            apiCounter += 1
        apiListFile.close()
    print("apiListFromFileDict =>", apiListFromFileDict)

    folderCounter = 0
    path = "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/storage/analyses/"
    for family, ids in sorted(familyDirectoryDict.items()):
        print("\nFamily + ids ==>" + str(family) + str(ids))
        for id in ids:
            file = open(path + str(id) + "/reports/report.json", 'r')  # , encoding='UTF-8')
            report = file.read()
            jsonInfo = json.loads(report)

            print("\n" + path + str(id) + "/reports/report.json")

            # append a new list for saving an apisequence.
            apiSequenceVector.append([])

            # originalProcess=False
            apiCounter = 0
            for process in jsonInfo['behavior']['processes']:
                # if "OriginalName_" not in process['process_path']:
                #     continue
                # else:
                #     originalProcess = True
                print(process['process_path'])

                first = 2
                for callInfo in process['calls']:
                    if first!=0:
                        print(callInfo['api'])
                        first -= 1

                    apiCall =  callInfo['api']
                    apiCallRemoved = removeNt_Ex_W_A(apiCall)
                    apiSequenceVector[folderCounter].append([])
                    apiSequenceVector[folderCounter][apiCounter].append(apiListFromFileDict[apiCallRemoved])
                    apiSequenceVector[folderCounter][apiCounter].append(callInfo['time'])
                    apiCounter += 1
                    # print("apiSequenceVector=>" + str(apiSequenceVector[folderCounter]))


            # if originalProcess==False:
            #     print("Cannot find a process with the same name!")
            print("BEFORE Sorted=>" + str(apiSequenceVector[folderCounter]))
            apiSequenceVector[folderCounter] = sorted(apiSequenceVector[folderCounter], key=itemgetter(1), reverse=False)
            print("AFTER Sorted=>" + str(apiSequenceVector[folderCounter]))
            apiSequenceVectorWithoutTime = []
            for api in apiSequenceVector[folderCounter]:
                apiSequenceVectorWithoutTime.append(api[0])
            apiSequenceVector[folderCounter] = apiSequenceVectorWithoutTime

            folderCounter += 1
            file.close()  # file = open(folder + "/reports/report.json", 'r')

    return apiSequenceVector


def createApiSequenceVectorByTimeWindow(familyDirectoryDict, timeWindowPeriod, refined):
    # This vector is used to store information of report.jsons
    apiSequenceVector = list()
    apiSequenceVectorTokenized = list()

    # Get the api list from 'apiList.txt'
    apiCounter = 0
    apiListFromFileDict = dict()

    if os.path.exists('apiList.txt'):
        apiListFile = open('apiList.txt', 'r')
        for api in apiListFile:
            apiListFromFileDict[api.rstrip()] = apiCounter
            apiCounter += 1
        apiListFile.close()
    print("apiListFromFileDict =>", apiListFromFileDict)

    apiTimeWindowIndicesDict = {}
    apiSequenceTimesDict = {}
    longestLen = -1
    # timeWindowPeriod = [0.25, 0.5, 1]    # Time window in second
    folderCounter = 0
    path = "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/storage/analyses/"
    for family, ids in sorted(familyDirectoryDict.items()):
        print("\nFamily + ids ==>" + str(family) + str(ids))
        for id in ids:
            file = open(path + str(id) + "/reports/report.json", 'r')  # , encoding='UTF-8')
            report = file.read()
            jsonInfo = json.loads(report)

            print("\n" + path + str(id) + "/reports/report.json")

            # append a new list for saving an apisequence.
            apiSequenceVector.append([])

            originalProcess=False
            for process in jsonInfo['behavior']['processes']:
                # if "OriginalName_" not in process['process_path']:
                #     continue
                # else:
                #     originalProcess = True
                print(process['process_path'])

                first = 2
                timeBase = -1
                tempApiSequence = []
                for callInfo in process['calls']:
                    if timeBase==-1:
                        timeBase = callInfo['time']

                    if first!=0:
                        print(callInfo['api'])
                        first -= 1

                    apiCall =  callInfo['api']
                    apiCallRemoved = removeNt_Ex_W_A(apiCall)
                    if (callInfo['time'] - timeBase) <= timeWindowPeriod:
                        tempApiSequence.append(apiListFromFileDict[apiCallRemoved])
                    else:
                        if refined:
                            tempApiSequence = removeRepetitionAPISeqOne(tempApiSequence)

                        if str(tempApiSequence) not in apiTimeWindowIndicesDict:
                            apiTimeWindowIndicesDict[str(tempApiSequence)] = len(apiTimeWindowIndicesDict) + 1

                        # print("tempApiSequence=>" + str(tempApiSequence))
                        # print("index=>" + str(apiTimeWindowIndicesDict[str(tempApiSequence)]) + "\n")
                        apiSequenceVector[folderCounter].append(apiTimeWindowIndicesDict[str(tempApiSequence)])
                        del tempApiSequence[:]
                        tempApiSequence.append(apiListFromFileDict[apiCallRemoved])
                        timeBase = callInfo['time']
                    # apiSequenceVector[folderCounter].append(apiListFromFileDict[apiCallRemoved])

                if tempApiSequence:
                    if refined:
                        tempApiSequence = removeRepetitionAPISeqOne(tempApiSequence)

                    if str(tempApiSequence) not in apiTimeWindowIndicesDict:
                        apiTimeWindowIndicesDict[str(tempApiSequence)] = len(apiTimeWindowIndicesDict) + 1

                    apiSequenceVector[folderCounter].append(apiTimeWindowIndicesDict[str(tempApiSequence)])
                    del tempApiSequence[:]
                    # tempApiSequence.append(apiListFromFileDict[apiCallRemoved])
                    # timeBase = callInfo['time']

            # if originalProcess==False:
            #     print("Cannot find a process with the same name!")
            # print("apiTimeWindowIndicesDict=>" + str(apiTimeWindowIndicesDict))
            print("apiSequenceVector[" + str(folderCounter) + "]=>" + str(apiSequenceVector[folderCounter]))

            #####>>> Tokenized >>>
            apiSequenceTimesDict.clear()
            apiSequenceVectorTokenized.append([])

            for item in apiSequenceVector[folderCounter]:
                if item not in apiSequenceTimesDict:
                    apiSequenceTimesDict[item] = 1
                else:
                    apiSequenceTimesDict[item] += 1

            now = 1
            for k, v in sorted(apiSequenceTimesDict.items()):
                print("k:" + str(k) + "  v:" + str(v))

                if k > now:
                    for i in xrange(k - now):
                        apiSequenceVectorTokenized[folderCounter].append(0)

                apiSequenceVectorTokenized[folderCounter].append(v)
                now = k + 1

            print("len:" + str(len(apiSequenceVectorTokenized[folderCounter])) + " apiSequenceVectorTokenized[" + str(folderCounter) + "]=>" + str(apiSequenceVectorTokenized[folderCounter]))
            if len(apiSequenceVectorTokenized[folderCounter]) >  longestLen:
                longestLen = len(apiSequenceVectorTokenized[folderCounter])
            #####___ Tokenized ___

            folderCounter += 1
            file.close()  # file = open(folder + "/reports/report.json", 'r')


    # Go through apiSequenceVectorTokenized to append 0 to members which don't have certain api functions of the last member
    for i in xrange(len(apiSequenceVectorTokenized)):
        print("\ni: " + str(i))
        #
        # print("Origin - " + str(len(apiVector[i])))
        # print(str(apiVector[i]))

        if len(apiSequenceVectorTokenized[i]) < longestLen:
            for j in xrange(longestLen - len(apiSequenceVectorTokenized[i])):
                apiSequenceVectorTokenized[i].append(0)

        print("len:" + str(len(apiSequenceVectorTokenized[i])) + "   " + str(apiSequenceVectorTokenized[i]) + "\n")

    return apiSequenceVectorTokenized


def createApiSequenceVectorWithArgument(familyDirectoryDict):
    # This vector is used to store information of report.jsons
    apiSequenceVector = list()

    # Get the api list from 'apiList.txt'
    apiCounter = 0
    apiListFromFileDict = dict()

    if os.path.exists('apiList.txt'):
        apiListFile = open('apiList.txt', 'r')
        for api in apiListFile:
            apiListFromFileDict[api.rstrip()] = apiCounter
            apiCounter += 1
        apiListFile.close()
    print("apiListFromFileDict =>", apiListFromFileDict)

    folderCounter = 0
    path = "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/storage/analyses/"
    for family, ids in sorted(familyDirectoryDict.items()):
        print("\nFamily + ids ==>" + str(family) + str(ids))
        for id in ids:
            file = open(path + str(id) + "/reports/report.json", 'r')  # , encoding='UTF-8')
            report = file.read()
            jsonInfo = json.loads(report)

            print("\n" + path + str(id) + "/reports/report.json")

            # append a new list for saving an apisequence.
            apiSequenceVector.append([])

            originalProcess=False
            for process in jsonInfo['behavior']['processes']:
                # if "OriginalName_" not in process['process_path']:
                #     continue
                # else:
                #     originalProcess = True
                print(process['process_path'])

                first = 2
                for callInfo in process['calls']:
                    if first!=0:
                        print(callInfo['api'])
                        first -= 1

                    apiCall =  callInfo['api']
                    apiArgCounter = len(callInfo['arguments'])
                    # print("## ArgCounter=>" + str(apiArgCounter))
                    apiCallRemoved = removeNt_Ex_W_A(apiCall)
                    apiSequenceVector[folderCounter].append(int(apiListFromFileDict[apiCallRemoved])*1000 + apiArgCounter)
                    # print("### 1000=>" + str(int(apiListFromFileDict[apiCallRemoved]) * 1000 + apiArgCounter))

            # if originalProcess==False:
            #     print("Cannot find a process with the same name!")
            # print("\n" + path + str(id) + "/reports/report.json==>" + str(apiSequenceVector[folderCounter]))

            folderCounter += 1
            file.close()  # file = open(folder + "/reports/report.json", 'r')

    return apiSequenceVector

def createApiSequenceVectorWithArgumentValue(familyDirectoryDict):
    # This vector is used to store information of report.jsons
    apiSequenceVector = list()

    # Get the api list from 'apiList.txt'
    apiCounter = 0
    apiListFromFileDict = dict()

    if os.path.exists('apiList.txt'):
        apiListFile = open('apiList.txt', 'r')
        for api in apiListFile:
            apiListFromFileDict[api.rstrip()] = apiCounter
            apiCounter += 1
        apiListFile.close()
    print("apiListFromFileDict =>", apiListFromFileDict)

    numClasses = 5
    maxSum = -1000
    folderCounter = 0
    path = "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/storage/analyses/"
    for family, ids in sorted(familyDirectoryDict.items()):
        print("\nFamily + ids ==>" + str(family) + str(ids))
        for id in ids:
            file = open(path + str(id) + "/reports/report.json", 'r')  # , encoding='UTF-8')
            report = file.read()
            jsonInfo = json.loads(report)

            print("\n" + path + str(id) + "/reports/report.json")

            # append a new list for saving an apisequence.
            apiSequenceVector.append([])

            originalProcess=False
            for process in jsonInfo['behavior']['processes']:
                # if "OriginalName_" not in process['process_path']:
                #     continue
                # else:
                #     originalProcess = True
                print(process['process_path'])

                first = 2
                for callInfo in process['calls']:
                    if first!=0:
                        print(callInfo['api'])
                        first -= 1

                    apiCall =  callInfo['api']
                    # apiArgCounter = len(callInfo['arguments'])
                    sum = 0
                    for k, v in sorted(callInfo['arguments'].items()):
                        # print("### Arguments=>", k, v, type(v))

                        if 'int' in str(type(v)):
                            sum += v
                        else:
                            try:
                                # elif str(v)[:2] == "0x":
                                sum += int(v, 16)
                            except:
                                if 'dict' in str(type(v)):
                                    for k2, v2 in sorted(v.items()):
                                        print("### k2 v2=>", k2, v2, type(v2))

                                        if 'int' in str(type(v2)):
                                            sum += v2
                                        else:
                                            try:
                                                sum += int(v2, 16)
                                            except:
                                                for letter in v2:
                                                    try:
                                                        # print("letter=>" + str(letter))
                                                        sum += (ord(letter))
                                                    except:
                                                        print("###dict Exception=>", letter)
                                elif 'list' in str(type(v)):
                                    for l in v:
                                        print("### l=>", l, type(l))

                                        if 'int' in str(type(l)):
                                            sum += l
                                        else:
                                            try:
                                                sum += int(l, 16)
                                            except:
                                                for letter in l:
                                                    try:
                                                        # print("letter=>" + str(letter))
                                                        sum += (ord(letter))
                                                    except:
                                                        print("###list Exception=>", letter)
                                else:
                                    for letter in v:
                                        try:
                                            # print("letter=>" + str(letter))
                                            sum += (ord(letter))
                                        except:
                                            print("### Exception=>", letter)

                    if sum > maxSum:
                        maxSum = sum
                    # print("### sum=>" + str(sum) + "   maxSum=>" + str(maxSum))

                    argValue = sum % numClasses
                    # for num in str(sum):
                    #     argValue += int(num)
                    # print("### argValue=>" + str(argValue))

                    # print("### Arguments=>" + str(callInfo['arguments']))
                    apiCallRemoved = removeNt_Ex_W_A(apiCall)
                    apiSequenceVector[folderCounter].append(int(apiListFromFileDict[apiCallRemoved])*numClasses + argValue)
                    # print("### 10000=>" + str(int(apiListFromFileDict[apiCallRemoved])*10000 + argValue))

            # if originalProcess==False:
            #     print("Cannot find a process with the same name!")
            # print("\n" + path + str(id) + "/reports/report.json==>" + str(apiSequenceVector[folderCounter]))

            folderCounter += 1
            file.close()  # file = open(folder + "/reports/report.json", 'r')

    print("### maxSum=>" + str(maxSum))

    return apiSequenceVector

def removeRepetitionAPISeqOne(apiSequence):
    maxLenRepeatedPattern = 4

    seqIndex = 0
    sequenceLen = len(apiSequence)
    while seqIndex < sequenceLen - 1:
        # print(apiSequenceVector[i][seqIndex])
        foundRepeatedPattern = False
        for lenRepetition in xrange(1, maxLenRepeatedPattern + 1):
            if foundRepeatedPattern:
                break
            else:
                apiSequence, foundRepeatedPattern, currentIndex = removeAPattern(apiSequence, seqIndex, lenRepetition)
                currentLenRepetition = lenRepetition

        if foundRepeatedPattern:
            seqIndex = currentIndex + currentLenRepetition
        else:
            seqIndex += 1

    apiSequence[:] = (value for value in apiSequence if value != -1)
    return apiSequence


def removeRepetitonAPISeq(apiSequenceVector):
    maxLenRepeatedPattern = 4

    for i in xrange(len(apiSequenceVector)):
        #print("\napiSequenceVector[" + str(i) + "] => " + str(apiSequenceVector[i]))
        seqIndex = 0
        sequenceLen = len(apiSequenceVector[i])
        while seqIndex < sequenceLen - 1:
            # print(apiSequenceVector[i][seqIndex])
            foundRepeatedPattern = False
            for lenRepetition in xrange(1, maxLenRepeatedPattern + 1):
                if foundRepeatedPattern:
                    break
                else:
                    apiSequenceVector[i], foundRepeatedPattern, currentIndex = removeAPattern(apiSequenceVector[i], seqIndex, lenRepetition)
                    currentLenRepetition = lenRepetition

            if foundRepeatedPattern:
                seqIndex = currentIndex + currentLenRepetition
            else:
                seqIndex += 1

            # print("\nIn while => len:" + str(sequenceLen) + "   seqIndex:" + str(seqIndex) + "  currentLenRepetition:" +
            #     str(currentLenRepetition) + "   currentIndex:" + str(currentIndex))
            #print(str(apiSequenceVector[i]) + "\n")

        #print("BEFOREIn removeRepetitionAPISeq => " + str(i) + " " + str(apiSequenceVector[i]))
        apiSequenceVector[i][:] = (value for value in apiSequenceVector[i] if value != -1)
        #print("AFTER In removeRepetitionAPISeq => " + str(i) + " " + str(apiSequenceVector[i]) + "\n")


    return apiSequenceVector


def removeAPattern(apiSequence, seqIndex, lenRepetition):
    pattern = apiSequence[seqIndex:(seqIndex + lenRepetition)]
    sequenceLen = len(apiSequence)
    foundRepeatedPattern = False
    currentIndex = -1

    for i in xrange(seqIndex + lenRepetition, sequenceLen - lenRepetition + 1, lenRepetition):
        if apiSequence[i:(i + lenRepetition)] == pattern:
            foundRepeatedPattern = True

            for j in xrange(i, i + lenRepetition):
                apiSequence[j] = -1

            currentIndex = i
        else:
            break

    return apiSequence, foundRepeatedPattern, currentIndex


# def pickSampleAndGenReport():
#     lenSamples = len(samples)
#     randomPick = randint(0, lenSamples-1)
#     #print(samples, sampleUsedDict)
#     #del sampleUsedDict[samples[randomPick]]
#
#     print(join(samplePath, samples[randomPick]))
#     # output = subprocess.check_output(["python", "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/utils/submit.py", join(samplePath, samples[randomPick])])
#     # if "Success" not in output:
#     #     sys.exit(output)
#     # else:
#     #     print(output)
#
#     REST_URL = "http://localhost:8090/tasks/create/file"
#     SAMPLE_FILE = join(samplePath, samples[randomPick])
#
#     with open(SAMPLE_FILE, "rb") as sample:
#         multipart_file = {"file": (samples[randomPick], sample)}
#         request = requests.post(REST_URL, files=multipart_file)
#
#     # Add your code to error checking for request.status_code.
#     print("request.status_code " + str(request.status_code))
#     if request.status_code != requests.codes.ok:
#         sys.exit("Submitting a sample to Cuckoo FAILED!")
#
#     json_decoder = json.JSONDecoder()
#     task_id = json_decoder.decode(request.text)["task_id"]
#     print("task_id " + str(task_id))
#
#     while True:
#         output = subprocess.check_output(["curl", "http://localhost:8090/tasks/view/" + str(task_id)])
#         print(output)
#         print("\n")
#         if "\"status\": \"reported\"" in output:
#             print("FINISHED!")
#             break
#         time.sleep(10)
#
#     # print(samples)
#     del samples[randomPick]
#     # print(samples)

# numberFromFamily  => take how many test samples from each family
# numberOfFamily    => how many families there are
# samplesOfFamily   => how many samples in each family
# def getTestVector(oriVector, numberFromFamily, numberOfFamily):
#     vectorType = str(type(oriVector))
#     sparseOrNot = False
#     if "sparse" in vectorType:
#         sparseOrNot = True
#
#     if sparseOrNot:
#         samplesOfFamily = oriVector.shape[0]/numberOfFamily
#         testVector = sp.lil_matrix((numberFromFamily*numberOfFamily, oriVector.shape[1]), dtype=np.float64)
#
#         testCounter = 0
#         for i in xrange(0, oriVector.shape[0], samplesOfFamily):
#             for j in xrange(numberFromFamily):
#                 testVector[testCounter] = oriVector[i+j]
#                 testCounter += 1
#     else:
#         samplesOfFamily = len(oriVector) / numberOfFamily
#         testVector = list()
#
#         for i in xrange(0, len(oriVector), samplesOfFamily):
#             for j in xrange(numberFromFamily):
#                 testVector.append(oriVector[i+j])
#
#     return testVector

# def printSeparate(label, predict, labelNum, preNumber):
#     labCounter = 0
#     preCounter = 0
#
#     while labCounter < len(label):
#         line = ""
#
#         for i in xrange(labelNum):
#             line += str(label[labCounter])
#             labCounter += 1
#
#         line += " "
#
#         for i in xrange(preNumber):
#             line += str(predict[preCounter])
#             preCounter += 1
#
#         print(line)

def printNumPerLine(input, numPerLine):
    print("input=>" + str(input) + "  len(input)=>" + str(len(input)))
    print("numPerLine=>" + str(numPerLine))

    counter = 0
    while counter < len(input):
        line = ""

        for i in xrange(numPerLine):
            line += str(input[counter])
            counter += 1

        print(line)

def selectSamples():
    familyDirectoryDict = dict()
    unqualifiedFamilyTask = dict()
    for name in malwareFamilies:
        familyDirectoryDict[name] = []
        unqualifiedFamilyTask[name] = []

    # print("familyDirectoryDict => " + str(familyDirectoryDict))

    taskList = subprocess.check_output(["curl", "http://localhost:8090/tasks/list"])
    json_decoder = json.JSONDecoder()
    tasks = json_decoder.decode(taskList)["tasks"]
    for task in tasks:
        for name in malwareFamilies:
            if name in task["target"]:
                familyDirectoryDict[name].append(task["id"])
                # print(task["id"], task["target"])

    print("BEFORE familyDirectoryDict =>" + str(familyDirectoryDict))

    path = "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/storage/analyses/"
    for family in familyDirectoryDict:
        copyList = list(familyDirectoryDict[family])
        for task in copyList:
            file = open(path + str(task) + "/reports/report.json", 'r')  # , encoding='UTF-8')
            report = file.read()
            jsonInfo = json.loads(report)

            if 'apistats' not in jsonInfo['behavior']:
                print("**ERROR REFINEMENT** " + str(task) + " has no apistats!")
                familyDirectoryDict[family].remove(task)
                file.close()
                continue

            apiStatsCounter = 0
            for pid in jsonInfo['behavior']['apistats'].keys():
                #for api in jsonInfo['behavior']['apistats'][pid].keys():
                apiStatsCounter += len(jsonInfo['behavior']['apistats'][pid].keys())
                if apiStatsCounter > 30:
                    break

            if apiStatsCounter <= 30:
                print("**ERROR REFINEMENT** " + family + " " + str(task) + " apiStatsCounter less than 30 => " + str(apiStatsCounter))
                familyDirectoryDict[family].remove(task)
                file.close()

                unqualifiedFamilyTask[family].append([task, apiStatsCounter])
                continue

            if 'processes' not in jsonInfo['behavior']:
                print("**ERROR REFINEMENT** " + str(task) + " has no processes!")
                familyDirectoryDict[family].remove(task)
                file.close()
                continue

            callsCounter = 0
            for process in jsonInfo['behavior']['processes']:
                #for api in jsonInfo['behavior']['apistats'][pid].keys():
                # print("process calls =>" + str(process['calls']))
                print("len(process calls) =>" + str(len(process['calls'])))
                callsCounter += len(process['calls'])
                if callsCounter > 100:
                    break

            if callsCounter <= 100:
                print("**ERROR REFINEMENT** " + family + " " + str(task) + " callsCounter less than 100 => " + str(callsCounter))
                familyDirectoryDict[family].remove(task)
                file.close()
                continue

            file.close()

        while len(familyDirectoryDict[family]) > numberFromEachFamily:
            del familyDirectoryDict[family][randint(0, len(familyDirectoryDict[family]) - 1)]

    print("unqualifiedFamilyTask => " + str(unqualifiedFamilyTask))

    # If still some families do not get enough samples, retake samples from trash
    for family in familyDirectoryDict.iterkeys():
        if len(familyDirectoryDict[family]) < numberFromEachFamily:
            need = numberFromEachFamily - len(familyDirectoryDict[family])

            sortedTasks = sorted(unqualifiedFamilyTask[family], key=itemgetter(1), reverse=True)
            for i in xrange(need):
                familyDirectoryDict[family].append(sortedTasks[i][0])

    print("familyDirectoryDict =>" + str(familyDirectoryDict))
    return familyDirectoryDict
    # analysisFolders = list()
    # for family in familyDirectoryDict:
    #     for folder in familyDirectoryDict[family]:
    #         analysisFolders.append(path + str(folder))
    #
    # print("analysisFolders => " + str(analysisFolders))
    # for family, ids in sorted(familyDirectoryDict.items()):
    #     print(family)
    #     print(ids)

def selectListFunction(numberFromEachFamily, clusterNum, trainingNumber, trainOrTest):
    selectList = list()

    if trainOrTest=="train":
        startNum = 0
        gap = trainingNumber
    else:
        startNum = trainingNumber
        gap = numberFromEachFamily - trainingNumber

    for i in xrange(startNum, numberFromEachFamily * clusterNum, numberFromEachFamily):
        for j in xrange(i, i + gap):
            selectList.append(j)

    return selectList


def vectorToStringVector(apiSequenceVector):
    apiSequenceStringVector = list()

    for i in xrange(len(apiSequenceVector)):
        apiSequenceStringVector.append(" ".join(str(x) for x in apiSequenceVector[i]))
        #apiSequenceStringVector.append([])
        #apiSequenceStringVector[i] = " ".join(str(x) for x in apiSequenceVector[i])

    return apiSequenceStringVector


def print_top_words(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print("Topic #%d:" % topic_idx)
        print("_".join([feature_names[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
    print()


def AgglomerativeGridSearchCVPlusCountVec(stringVector, yTargetArray, stringVectorTest, yTestTargetArray, percentTop, knnBottom, knnTop, clusterNum):
    bestParamsPlusNGram = dict()

    for numGram in xrange(3, 6):
        print("\n*** numGram *** => " + str(numGram))
        ngram_vectorizer = CountVectorizer(ngram_range=(numGram, numGram), token_pattern=r'\b\w+\b', min_df=1)  # \w => [A-Za-z0-9_]
        ngram_vectorizerFit = ngram_vectorizer.fit(stringVector)
        stringVectorGrammedArray = ngram_vectorizerFit.transform(stringVector).toarray()
        stringVectorTestGrammedArray = ngram_vectorizerFit.transform(stringVectorTest).toarray()

        if not bestParamsPlusNGram:
            bestParamsPlusNGram = AgglomerativeGridSearchCV(stringVectorGrammedArray, yTargetArray, stringVectorTestGrammedArray, yTestTargetArray, percentTop, knnBottom, knnTop, clusterNum)
            bestParamsPlusNGram['agglo_ngram'] = numGram
        else:
            tempBestParams = AgglomerativeGridSearchCV(stringVectorGrammedArray, yTargetArray, stringVectorTestGrammedArray, yTestTargetArray, percentTop, knnBottom, knnTop, clusterNum)
            if tempBestParams['agglo_predictScore'] > bestParamsPlusNGram['agglo_predictScore']:
                bestParamsPlusNGram = tempBestParams
                bestParamsPlusNGram['agglo_ngram'] = numGram

    return bestParamsPlusNGram

def AgglomerativeGridSearchCV(vectorArray, yTargetArray, vectorTestArray, yTestTargetArray, percentTop, knnBottom, knnTop, clusterNum):
    transformer = TfidfTransformer(smooth_idf=True)
    # vectorArrayTFIDF = transformer.fit_transform(vectorArray)
    # vectorTestArrayTFIDF = transformer.fit_transform(vectorTestArray)

    knnList = list(xrange(knnBottom, knnTop+1))
    knnList.append(None)

    bestScore = -1
    bestParams = dict()
    cv = StratifiedKFold(n_splits=3)
    # yTargetArray = numpy.array(yTarget)
    # yTestTargetArray = numpy.array(yTestTarget)

    for percent in xrange(percentTop, 0, -1):
        # vectorFit = SelectPercentile(f_classif, percentile=percent).fit(vectorArrayTFIDF, yTargetArray)
        # vectorTFIDFPercent = vectorFit.transform(vectorArrayTFIDF)
        # vectorTestTFIDFPercent = vectorFit.transform(vectorTestArrayTFIDF)

        for knn in knnList:
            for linkage in ['average', 'complete', 'ward']:
                overallScore = 0

                for train_index, test_index in cv.split(vectorArray, yTargetArray):
                    # print("\n")
                    # print("TRAIN:" + str(train_index) + "\nTEST:" + str(test_index))
                    X_train, X_test = vectorArray[train_index], vectorArray[test_index]
                    y_train, y_test = yTargetArray[train_index], yTargetArray[test_index]

                    X_trainTFIDF = transformer.fit_transform(X_train)
                    X_trainFit = SelectPercentile(f_classif, percentile=percent).fit(X_trainTFIDF, y_train)
                    X_trainTFIDFPercent = X_trainFit.transform(X_trainTFIDF)

                    # print("percent=>" + str(percent))
                    # print("knn=>" + str(knn))
                    if knn != None:
                        knn_graph = kneighbors_graph(X_trainTFIDFPercent, knn, include_self=False)
                        model = AgglomerativeWithTransofrm(linkage=linkage,
                                                           connectivity=knn_graph,
                                                           n_clusters=clusterNum)
                    else:
                        model = AgglomerativeWithTransofrm(linkage=linkage,
                                                           n_clusters=clusterNum)

                    model.fit(X_trainTFIDFPercent.todense())
                    # print("model.labels_=>" + str(model.labels_))
                    # print("score=>" + str(v_measure_score(y_train, model.labels_)))

                    X_testTFIDF = transformer.fit_transform(X_test)
                    X_testTFIDFPercent = X_trainFit.transform(X_testTFIDF)
                    predictScore = v_measure_score(y_test, model.predict(X_testTFIDFPercent.todense()))
                    # print("predict_score=>" + str(predictScore))

                    overallScore += predictScore

                if overallScore / 3.0 > bestScore:
                    bestScore = overallScore / 3.0

                    bestParams['agglo_percent'] = percent
                    bestParams['agglo_knn'] = knn
                    bestParams['agglo_linkage'] = linkage
                    bestParams['agglo_score'] = bestScore

                    # print("\n\nBEST percent=>" + str(percent))
                    # print("BEST knn=>" + str(knn))
                    # print("BEST linkage=>" + str(linkage))
                    # print("BEST score=>" + str(bestScore))

    print("\n\nAll BEST percent=>" + str(bestParams['agglo_percent']))
    print("All BEST knn=>" + str(bestParams['agglo_knn']))
    print("All BEST linkage=>" + str(bestParams['agglo_linkage']))
    print("All BEST score=>" + str(bestParams['agglo_score']))

    vectorArrayTFIDF = transformer.fit_transform(vectorArray)
    vectorTestArrayTFIDF = transformer.fit_transform(vectorTestArray)
    vectorArrayFit = SelectPercentile(f_classif, percentile=bestParams['agglo_percent']).fit(vectorArrayTFIDF, yTarget)
    vectorTestArrayTFIDFPercent = vectorArrayFit.transform(vectorTestArrayTFIDF)

    if bestParams['agglo_knn'] != None:
        knn_graph = kneighbors_graph(vectorTestArrayTFIDFPercent, bestParams['agglo_knn'], include_self=False)
        model = AgglomerativeWithTransofrm(linkage=bestParams['agglo_linkage'],
                                           connectivity=knn_graph,
                                           n_clusters=clusterNum)
    else:
        model = AgglomerativeWithTransofrm(linkage=bestParams['agglo_linkage'],
                                           n_clusters=clusterNum)

    bestParams['agglo_predictScore'] = v_measure_score(yTestTargetArray, model.fit(vectorTestArrayTFIDFPercent.todense()).labels_)
    print("All BEST test score=>" + str(bestParams['agglo_predictScore']))

    return bestParams


def getSelectedSamplesNames(familyDirectoryDict):
    taskList = subprocess.check_output(["curl", "http://localhost:8090/tasks/list"])
    json_decoder = json.JSONDecoder()
    tasks = json_decoder.decode(taskList)["tasks"]
    idFileNameDict = dict()

    for task in tasks:
        idFileNameDict[task["id"]] = task["target"]

    print
    for family, ids in sorted(familyDirectoryDict.items()):
        #print("\nFamily + ids ==>" + str(family) + str(ids))

        for id in ids:
            #print(str(id) + "   " + str(idFileNameDict[id]))
            print(str(idFileNameDict[id]))
        print

def select25SamplesFrom50():
    for family, ids in sorted(familyDirectoryDict_50.items()):
        print("\nFamily + ids ==>" + str(family) + str(ids))

        startNum = randint(0, 25)
        for i in xrange(startNum, startNum+25):
            print str(ids[i]) + ",",
        print

def processSamplesMalheur():
    counter = 0
    finishedList = ["andromeda", "asprox", "citadel", "gameover", "luxnet", "neverquest", "tinba", "upatre"]

    with open("malheurSamplesList.txt", 'rb') as rF:
        for line in rF:
            #curl -F file=@/path/to/file http://localhost:8090/tasks/create/file
            REST_URL = "http://localhost:8090/tasks/create/file"
            SAMPLE_FILE = line.strip()

            if SAMPLE_FILE == "":
                continue

            skip = False
            for finished in finishedList:
                if finished in SAMPLE_FILE:
                    skip = True
                    break
            if skip:
                continue

            print(SAMPLE_FILE)

            with open(SAMPLE_FILE, "rb") as sample:
                multipart_file = {"file": ("temp_file_name", sample)}
                request = requests.post(REST_URL, files=multipart_file)

            # Add your code to error checking for request.status_code.
            json_decoder = json.JSONDecoder()
            task_id = json_decoder.decode(request.text)["task_ids"]

            # Add your code for error checking if task_id is None.
            print(request.text)

            counter += 1
            if counter == 150:
                break


class DenseTransformer(BaseEstimator, TransformerMixin): # Multiple Inheritance => class DerivedClassName(Base1, Base2, Base3):
    def transform(self, X, y=None, **fit_params):
        return X.todense()

    def fit_transform(self, X, y=None, **fit_params):
        self.fit(X, y, **fit_params)
        return self.transform(X)

    def fit(self, X, y=None, **fit_params):
        return self

class DBScanWithTransofrm(cluster.DBSCAN, TransformerMixin):
    def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
                 algorithm='auto', leaf_size=30, p=None, n_jobs=1, n_neighbors=3):
        super(DBScanWithTransofrm, self).__init__(eps, min_samples, metric, algorithm, leaf_size, p, n_jobs)
        self.n_neighbors = n_neighbors

    # def transform(self, X, y=None):                     # This is a dummy transform functiono just for GridSearchCV
    #     return self.fit(X).labels_
    #
    # def fit_transform(self, X, y=None, **fit_params):   # This is a dummy transform functiono just for GridSearchCV
    #     self.fit(X, y, **fit_params)
    #     return self.fit(X).labels_

    def fit(self, X, y=None, sample_weight=None):
        # print(super(DBScanWithTransofrm, self).get_params())
        self = super(DBScanWithTransofrm, self).fit(X, y, sample_weight)
        # print("111 self.labels_=>" + str(self.labels_))
        self.copyX = X
        return self

    def predict(self, X):
        neighbor = KNeighborsClassifier(n_neighbors=self.n_neighbors)
        # print("222 self.labels_=>" + str(self.labels_))
        # print(np.unique(self.labels_), max(self.labels_))
        self.labels_ [self.labels_ == -1] = max(self.labels_) + 1
        neighbor.fit(self.copyX, self.labels_)
        # print("self.copyX=>" + str(self.copyX))
        # print("X=>" + str(X))
        # print("self.labels_=>" + str(self.labels_))
        return neighbor.predict(X)

class AgglomerativeWithTransofrm(AgglomerativeClustering, TransformerMixin):
    def __init__(self,  n_clusters=2, affinity="euclidean",
                        memory=Memory(cachedir=None, verbose=0),
                        connectivity=None, compute_full_tree='auto',
                        linkage='ward', pooling_func=np.mean):
        super(AgglomerativeWithTransofrm, self).__init__(n_clusters, affinity, memory, connectivity, compute_full_tree, linkage, pooling_func)
        self.n_neighbors = 5
        self.n_clusters = n_clusters
        self.connectivity = connectivity
        self.linkage = linkage
        # def transform(self, X, y=None):                     # This is a dummy transform functiono just for GridSearchCV
        #     return self.fit(X).labels_
        #
        # def fit_transform(self, X, y=None, **fit_params):   # This is a dummy transform functiono just for GridSearchCV
        #     self.fit(X, y, **fit_params)
        #     return self.fit(X).labels_

    def fit(self, X, y=None):
        # print(super(DBScanWithTransofrm, self).get_params())
        self = super(AgglomerativeWithTransofrm, self).fit(X, y)
        # print("111 self.labels_=>" + str(self.labels_))
        self.copyX = X
        return self

    def predict(self, X):
        neighbor = KNeighborsClassifier(n_neighbors=self.n_neighbors)
        # print("222 self.labels_=>" + str(self.labels_))
        # print(np.unique(self.labels_), max(self.labels_))
        self.labels_[self.labels_ == -1] = max(self.labels_) + 1
        neighbor.fit(self.copyX, self.labels_)
        # print("self.copyX=>" + str(self.copyX))
        # print("X=>" + str(X))
        # print("self.labels_=>" + str(self.labels_))
        return neighbor.predict(X)

# def _num_samples(x):
#     """Return number of samples in array-like x."""
#     if hasattr(x, 'fit'):
#         # Don't get num_samples from an ensembles length!
#         raise TypeError('Expected sequence or array-like, got '
#                         'estimator %s' % x)
#     if not hasattr(x, '__len__') and not hasattr(x, 'shape'):
#         if hasattr(x, '__array__'):
#             x = np.asarray(x)
#         else:
#             raise TypeError("Expected sequence or array-like, got %s" %
#                             type(x))
#     if hasattr(x, 'shape'):
#         if len(x.shape) == 0:
#             raise TypeError("Singleton array %r cannot be considered"
#                             " a valid collection." % x)
#         return x.shape[0]
#     else:
#         return len(x)
#
# def check_consistent_length(*arrays):
#     for X in arrays:
#         print(X)
#     lengths = [_num_samples(X) for X in arrays if X is not None]
#     uniques = np.unique(lengths)
#     print("lengths=>" + str(lengths))
#     print("uniques=>" + str(uniques))
#     print("[int(l) for l in lengths]=>" + str([int(l) for l in lengths]))


if __name__ == "__main__":
    # processSamplesMalheur()
    # sys.exit("ERROR")
    # #####################

    pd.set_option('display.height', 1000)
    pd.set_option('display.max_rows', 500)
    pd.set_option('display.max_columns', 500)
    pd.set_option('display.width', 1000)

    arguments = sys.argv

    # Remove apiList.txt to reset api locations
    # *** If 'y', remember to upload "cuckooAnalysis.py", "apiList.txt", "apiVector.txt", "apiSequenceVector.txt" and "apiSequenceVectorTime.txt" to PVM. ***
    #deleteApiList = raw_input("Delete apiList.txt (y/n)? (If using different samples, please delete!) ")
    deleteApiList = 'n'
    if deleteApiList=='y' and os.path.exists('apiList.txt'):
        os.remove("apiList.txt")

    # if os.path.exists('result.txt'):
    #     os.remove("result.txt")

    # Randomly pick a sample to generate the report
    # global samples, samplePath
    # samplePath = "/home/" + str(getpass.getuser()) + "/Downloads/cuckoo/samples/04.11_samples/unzip"
    # samples = [f for f in listdir(samplePath) if isfile(join(samplePath, f))]
    # pickSampleAndGenReport()
    # sys.exit("Error message")

    #>>> Make this process less likely to be killed >>>
    #
    f = os.popen("ps aux | grep -i \"python cuckooAnalysis.py\" | head -n1 | awk '{print $2}'")
    process = f.read().rstrip()

    processScore = subprocess.check_output(["cat", "/proc/" + str(process) + "/oom_score"])
    print("cat /proc/" + str(process) + "/oom_score=>" + str(processScore.rstrip()))

    # subprocess.check_output(["sudo", "echo", "-15" , ">" , "/proc/" + str(process) + "/oom_adj"])
    os.system("sudo echo -15 > /proc/" + str(process) + "/oom_adj")
    # f = os.open("sudo echo -15 > /proc/" + str(process) + "/oom_adj")

    processScore = subprocess.check_output(["cat", "/proc/" + str(process) + "/oom_score"])
    print("cat /proc/" + str(process) + "/oom_score=>" + str(processScore.rstrip()))
    #
    # <<< Make this process less likely to be killed <<<


    ##########
    # Manage malware families. Pick a certain number of samples from each family.
    # Remove reports which have no apistats or processes.
    global malwareFamilies, numberFromEachFamily
    #malwareFamilies = ["andromeda", "asprox", "citadel", "cybergate", "goznym", "poisonivy", "tinba", "upatre", "xswkit"]
    malwareFamilies = ["andromeda", "asprox", "citadel",  "gameover", "luxnet", "neverquest", "tinba", "upatre", "xswkit", "zeus"]
    # malwareFamilies = ['andromeda', 'asprox', 'cabart', 'citadel', 'corebot', 'cybergate', 'darkcomet', 'gameover', 'geodo',
    #                    'gozi', 'goznym', 'kins', 'locky', 'luxnet', 'matsnu', 'matsnu2015', 'neverquest', 'nymaim', 'panda',
    #                    'poison-ivy', 'poisonivy', 'pony', 'rovnix', 'sality', 'simda', 'tinba', 'tinba2', 'upatre', 'urlzone',
    #                    'vawtrak', 'xswkit', 'zeus']
    numberFromEachFamily = 50
    clusterNum = len(malwareFamilies)
    trainingNumber = 40

    # familyDirectoryDict = dict()
    # familyDirectoryDict = selectSamples()
    #
    # print("familyDirectoryDict =>" + str(familyDirectoryDict))
    # sys.exit("END")
    #
    ##########


    # For test, directly assign samples
    familyDirectoryDict_2 = {
        'andromeda': [34, 35],
        'asprox': [133, 134],
        'citadel': [183, 184],
        'gameover': [349, 351],
        'luxnet': [898, 902],
        'neverquest': [426, 427],
        'tinba': [606, 607],
        'upatre': [666, 668],
        'xswkit': [745, 746],
        'zeus': [809, 811]
    }

    familyDirectoryDict_5 = {
    'andromeda': [26, 32, 33, 34, 35],
    'asprox':    [125, 127, 129, 133, 134],
    'citadel':   [179, 181, 182, 183, 184],
    'gameover':  [344, 346, 348, 349, 351],
    'luxnet':    [895, 896, 897, 898, 902],
    'neverquest':[423, 424, 425, 426, 427],
    'tinba':     [603, 604, 605, 606, 607],
    'upatre':    [660, 662, 663, 666, 668],
    'xswkit':    [740, 741, 743, 745, 746],
    'zeus':      [805, 806, 808, 809, 811]
    }

    familyDirectoryDict_25 = {
    'andromeda': [26, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 56, 59, 62, 63, 64],
    'asprox':    [125, 127, 129, 133, 134, 135, 139, 140, 141, 143, 147, 148, 149, 151, 154, 155, 156, 157, 158, 159, 150, 138, 132, 88, 99],
    'citadel':   [179, 181, 182, 183, 184, 185, 186, 187, 188, 190, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209],
    'gameover':  [344, 346, 348, 349, 351, 354, 355, 357, 358, 359, 360, 361, 364, 365, 368, 369, 373, 374, 375, 377, 378, 383, 385, 386, 387],
    'luxnet':    [895, 896, 897, 898, 902, 904, 905, 906, 908, 909, 912, 913, 915, 917, 918, 919, 921, 922, 926, 927, 928, 929, 930, 933, 934],
    'neverquest':[423, 424, 425, 426, 427, 428, 430, 431, 433, 435, 437, 440, 443, 447, 449, 450, 451, 452, 453, 455, 456, 458, 459, 461, 463],
    'tinba':     [603, 604, 605, 606, 607, 609, 610, 611, 612, 613, 614, 615, 618, 619, 622, 623, 625, 627, 630, 631, 634, 635, 636, 637, 639],
    'upatre':    [660, 662, 663, 666, 668, 669, 670, 671, 674, 675, 677, 678, 679, 681, 684, 686, 687, 689, 693, 694, 695, 696, 699, 700, 701],
    'xswkit':    [740, 741, 743, 745, 746, 747, 748, 749, 750, 753, 754, 756, 757, 758, 759, 760, 761, 763, 766, 768, 769, 770, 774, 776, 777],
    'zeus':      [805, 806, 808, 809, 811, 812, 815, 816, 819, 820, 821, 822, 823, 825, 826, 827, 828, 830, 832, 834, 836, 837, 838, 841, 844]
    }

    familyDirectoryDict_50 = {
    'andromeda': [1, 6, 7, 8, 9, 10, 11, 12, 16, 17, 19, 20, 24, 25, 26, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42,
                 43, 44, 45, 46, 47, 48, 52, 53, 54, 56, 59, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 74, 76, 79, 80],
    'asprox':    [81, 82, 83, 85, 88, 99, 90, 91, 93, 94, 96, 97, 98, 100, 104, 106, 109, 110, 111, 112, 113, 114, 115, 117,
              119, 121, 123, 125, 127, 129, 133, 134, 135, 139, 140, 141, 143, 147, 148, 149, 151, 154, 155, 156,
              157, 158, 159, 150, 138, 132],
    'citadel':   [161, 163, 166, 168, 169, 170, 174, 176, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 190, 193,
                194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 212, 214, 215, 216, 217, 218,
                219, 222, 223, 224, 229, 232, 234, 236, 237, 239],
    'gameover':  [321, 323, 324, 325, 326, 327, 328, 330, 331, 332, 334, 335, 336, 338, 340, 341, 342, 343, 344,
                 346, 348, 349, 351, 354, 355, 357, 358, 359, 360, 361, 364, 365, 368, 369, 373, 374, 375, 377,
                 378, 383, 385, 386, 387, 388, 390, 391, 393, 396, 399, 400],
    'luxnet':    [881, 883, 884, 885, 886, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 902, 904, 905, 906, 908,
               909, 912, 913, 915, 917, 918, 919, 921, 922, 926, 927, 928, 929, 930, 933, 934, 935, 936, 940, 944,
               945, 946, 947, 948, 950, 952, 956, 957, 959, 960],
    'neverquest':[401, 406, 408, 409, 410, 411, 412, 413, 414, 415, 419, 420, 423, 424, 425, 426, 427, 428, 430,
                   431, 433, 435, 437, 440, 443, 447, 449, 450, 451, 452, 453, 455, 456, 458, 459, 461, 463, 465,
                   466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478],
    'tinba':     [562, 565, 566, 568, 570, 573, 574, 575, 577, 578, 579, 580, 582, 585, 587, 589, 590, 591, 592, 594,
              597, 599, 600, 601, 602, 603, 604, 605, 606, 607, 609, 610, 611, 612, 613, 614, 615, 618, 619, 622,
              623, 625, 627, 630, 631, 634, 635, 636, 637, 639],
    'upatre':    [641, 642, 644, 645, 648, 649, 654, 655, 656, 658, 659, 660, 662, 663, 666, 668, 669, 670, 671, 674,
               675, 677, 678, 679, 681, 684, 686, 687, 689, 693, 694, 695, 696, 699, 700, 701, 702, 703, 704, 705,
               706, 707, 708, 711, 712, 713, 714, 716, 718, 719],
    'xswkit':    [721, 723, 725, 730, 732, 735, 736, 737, 738, 739, 740, 741, 743, 745, 746, 747, 748, 749, 750, 753,
               754, 756, 757, 758, 759, 760, 761, 763, 766, 768, 769, 770, 774, 776, 777, 778, 780, 782, 783, 784,
               786, 787, 791, 793, 794, 795, 796, 797, 799, 800],
    'zeus':      [802, 804, 805, 806, 808, 809, 811, 812, 815, 816, 819, 820, 821, 822, 823, 825, 826, 827, 828, 830,
             832, 834, 836, 837, 838, 841, 844, 845, 846, 847, 850, 852, 854, 858, 859, 860, 861, 862, 863, 864,
             865, 868, 871, 874, 875, 876, 877, 878, 879, 880]
    }

#     familyDirectoryDict = {
# 'andromeda':  [148, 151, 152, 153, 154, 155, 157, 158, 160, 161, 162, 163, 164, 167, 168, 169, 170, 171, 172, 174, 179, 181, 182, 184, 187],
# 'asprox':     [189, 190, 191, 193, 194, 195, 271, 272, 273, 274, 275, 276, 278, 281, 282, 283, 453, 454, 455, 457, 460, 461, 462, 464, 465],
# 'citadel':    [198, 202, 204, 207, 284, 285, 286, 287, 288, 290, 291, 293, 296, 391, 395, 396, 398, 399, 421, 423, 199, 200, 201, 203, 205],
# 'cybergate':  [208, 209, 217, 299, 305, 401, 404, 406, 407, 409, 312, 212, 307, 211, 214, 216, 301, 303, 306, 308, 310, 402, 403, 408, 213],
# 'goznym':     [221, 223, 225, 227, 218, 316, 317, 318, 319, 320, 323, 325, 326, 328, 412, 413, 415, 416, 417, 420, 446, 447, 448, 449, 450],
# 'poisonivy':  [232, 236, 237, 329, 330, 331, 333, 337, 339, 342, 343, 427, 432, 433, 435, 440, 445, 230, 338, 436, 441, 429, 341, 428, 234],
# 'tinba':      [238, 240, 245, 247, 344, 346, 347, 350, 352, 353, 354, 356, 357, 358, 469, 470, 473, 474, 475, 476, 477, 478, 479, 480, 481],
# 'uptare':     [248, 249, 251, 252, 255, 359, 361, 363, 364, 365, 366, 367, 369, 370, 372, 482, 484, 485, 487, 489, 490, 491, 492, 494, 495],
# 'xswkit':     [260, 262, 263, 264, 265, 378, 379, 380, 383, 384, 385, 386, 389, 390, 497, 498, 500, 501, 502, 503, 507, 508, 509, 510, 511]
#     }

    if numberFromEachFamily==2:
        familyDirectoryDict = familyDirectoryDict_2
    elif numberFromEachFamily==5:
        familyDirectoryDict = familyDirectoryDict_5
    elif numberFromEachFamily==25:
        familyDirectoryDict = familyDirectoryDict_25
    elif numberFromEachFamily==50:
        familyDirectoryDict = familyDirectoryDict_50

    #####################
    ##getSelectedSamplesNames(familyDirectoryDict_50)
    # processSamplesMalheur()
    # sys.exit("ERROR")
    #####################


    # Generate the target data
    yTarget = []
    for i in xrange(len(malwareFamilies)):
        for j in xrange(trainingNumber):
            yTarget.append(i)

    yTestTarget = []
    for i in xrange(len(malwareFamilies)):
        for j in xrange(numberFromEachFamily-trainingNumber):
            yTestTarget.append(i)


    # >>> Select algorithms >>>
    # PYTHON False True 50 KMeans MeanShift AgglomerativeTrans DBScanTrans
    argumentPercent = int(arguments[3])
    clusterAlgorithm = list()
    if len(arguments)==4:
        clusterAlgorithm.append([])
        clusterAlgorithm[0].append('AgglomerativeTrans')
        clusterAlgorithm[0].append(AgglomerativeWithTransofrm())
        clusterAlgorithm[0].append(0)
        clusterAlgorithm.append([])
        clusterAlgorithm[1].append('DBScanTrans')
        clusterAlgorithm[1].append(DBScanWithTransofrm())
        clusterAlgorithm[1].append(1)
        clusterAlgorithm.append([])
        clusterAlgorithm[2].append('KMeans')
        clusterAlgorithm[2].append(cluster.KMeans())
        clusterAlgorithm[2].append(2)
        clusterAlgorithm.append([])
        clusterAlgorithm[3].append('MeanShift')
        clusterAlgorithm[3].append(cluster.MeanShift())
        clusterAlgorithm[3].append(3)
    else:
        for i in xrange(4, len(arguments)):
            clusterAlgorithm.append([])

            if 'AgglomerativeTrans' in arguments[i]:
                clusterAlgorithm[i-4].append('AgglomerativeTrans')
                clusterAlgorithm[i-4].append(AgglomerativeWithTransofrm())
            elif 'DBScanTrans' in arguments[i]:
                clusterAlgorithm[i-4].append('DBScanTrans')
                clusterAlgorithm[i-4].append(DBScanWithTransofrm())
            elif 'KMeans' in arguments[i]:
                clusterAlgorithm[i-4].append('KMeans')
                clusterAlgorithm[i-4].append(cluster.KMeans())
            elif 'MeanShift' in arguments[i]:
                clusterAlgorithm[i-4].append('MeanShift')
                clusterAlgorithm[i-4].append(cluster.MeanShift())
            elif 'BIRCH' in arguments[i]:
                clusterAlgorithm[i-4].append('BIRCH')
                clusterAlgorithm[i-4].append(cluster.Birch())

            clusterAlgorithm[i-4].append(i-4)

    print("clusterAlgorithm=>" + str(clusterAlgorithm))

    dataHistogram = np.array([['', 'Training Score', 'Prediction Score', 'Homogeneity', 'Completeness', 'Running Time', 'SelectPercentile']
                        , ['', '', '', '', '', '', '']
                        , ['', '', '', '', '', '', '']
                        , ['', '', '',  '', '','', '']
                        , ['', '', '',  '', '','', '']])
    dataSequence = np.array([['', 'Training Score', 'Prediction Score', 'Homogeneity', 'Completeness', 'Running Time', 'SelectPercentile', 'N-gram']
                                 , ['', '', '', '', '', '', '', '']
                                 , ['', '', '', '', '', '', '', '']
                                 , ['', '', '', '', '', '', '', '']
                                 , ['', '', '', '', '', '', '', '']])
    algorithmsBestParamsDict = []
    algorithmsBestParamsSequenceDict = []

    counter = 1
    for algorithm in clusterAlgorithm:
        dataHistogram[counter][0] = algorithm[0]
        dataSequence[counter][0] = algorithm[0]
        algorithmsBestParamsDict.append({})
        algorithmsBestParamsSequenceDict.append({})
        counter += 1

    print("dataHistogram=>" + str(dataHistogram))
    print("dataSequence=>" + str(dataSequence))
    print("algorithmsBestParamsDict=>" + str(algorithmsBestParamsDict) + "\n")
    # algorithmsBestParamsDict[0]['a'] = 'b'
    # algorithmsBestParamsDict.append({})
    # algorithmsBestParamsDict[1]['c'] = 'd'
    # print(algorithmsBestParamsDict)
    # print(dataHistogram, dataSequence)
    # print(pd.DataFrame(data=dataHistogram[1:, 1:], index=dataHistogram[1:, 0], columns=dataHistogram[0, 1:]))
    # print(pd.DataFrame(data=dataSequence[1:, 1:], index=dataSequence[1:, 0], columns=dataSequence[0, 1:]))
    #
    # <<< Select algorithms <<<


    ##########
    # create an api vector of how many times an api function is called
    if arguments[1]=='True':
        if deleteApiList=='y':

            apiVectorAll = createApiVector(familyDirectoryDict)

            with open("apiVector_" + str(numberFromEachFamily) + ".txt", 'wb') as aV:
                pickle.dump(apiVectorAll, aV)
        else:
            with open("apiVector_" + str(numberFromEachFamily) + ".txt", 'rb') as aV:
                apiVectorAll = pickle.load(aV)


        apiVectorAllArray = numpy.array(apiVectorAll)
        selectList = selectListFunction(numberFromEachFamily, clusterNum, trainingNumber, "train")
        apiVectorArray = apiVectorAllArray[selectList, ]
        print("apiVectorAllArray => \n" + str(apiVectorAllArray))
        print("apiVectorArray => \n" + str(apiVectorArray))
        print("apiVectorArray numberFromEachFamily:" + str(numberFromEachFamily) + " shape => " + str(apiVectorArray.shape))
        transformer = TfidfTransformer(smooth_idf=True)
        print("apiVectorArray TFIDF shape:" + str(transformer.fit_transform(apiVectorArray).toarray().shape))


        if True:
            selectList = selectListFunction(numberFromEachFamily, clusterNum, trainingNumber, "test")
            apiVectorTestArray = apiVectorAllArray[selectList, ]

            for algorithm in clusterAlgorithm:
                startTime = time.time()
                print("\n>>> API Vector " + str(algorithm[0]) + ">>>")
                if 'KMeans' in algorithm[0]:
                    estimators = [('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()), (algorithm[0], algorithm[1])] # ('lDA', LatentDirichletAllocation())
                    params = dict(tfIdf__smooth_idf=[True],
                                  # sPT__score_func= [chi2, f_classif, mutual_info_classif], sPT__percentile=range(100, 0, -1),
                                  sPT__score_func=[f_classif], sPT__percentile=range(argumentPercent, 0, -1),
                    # lDA__n_topics=[clusterNum], lDA__max_iter=[10], lDA__learning_method=['online'], lDA__learning_offset=[50.], lDA_random_state=[0])
                                  KMeans__n_clusters=[clusterNum], KMeans__random_state=[0], KMeans__precompute_distances=[True])
                elif 'MeanShift' in algorithm[0]:
                    estimators = [('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()), ('to_dense', DenseTransformer()), (algorithm[0], algorithm[1])]
                    params = dict(tfIdf__smooth_idf=[True],
                                  sPT__score_func=[f_classif], sPT__percentile=range(argumentPercent, 0, -1))
                elif 'BIRCH' in algorithm[0]:
                    estimators = [('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()), (algorithm[0], algorithm[1])]
                    params = dict(tfIdf__smooth_idf=[True],
                                  sPT__score_func=[f_classif], sPT__percentile=range(argumentPercent, 0, -1),
                                  BIRCH__n_clusters=[clusterNum])
                elif 'DBScanTrans' in algorithm[0]:
                    estimators = [('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()), (algorithm[0], algorithm[1])] #, ('kNC', KNeighborsClassifier())]
                    params = dict(tfIdf__smooth_idf=[True],
                                  sPT__score_func=[f_classif], sPT__percentile=range(argumentPercent, 0, -1),
                                  DBScanTrans__eps=np.arange(0.1, 2.8, 0.3), DBScanTrans__min_samples=range(5, 8), DBScanTrans__n_neighbors=[5])
                                  # DBScanTrans__eps = np.arange(0.1, 0.4, 0.3), DBScanTrans__min_samples = range(5, 6), DBScanTrans__n_neighbors = [5])
                                  # sPT__score_func=[f_classif], sPT__percentile=range(41, 40, -1),
                                  # DBScanTrans__eps=np.arange(9.1, 9.3, 0.1), DBScanTrans__min_samples=range(1, 2))#, DBScanTrans__n_neighbors=[3, 4, 5])
                                  #kNC__n_neighbors=[3, 4])
                elif 'AgglomerativeTrans' in algorithm[0]:
                    AggloBestParams = AgglomerativeGridSearchCV(apiVectorArray, numpy.array(yTarget), apiVectorTestArray, numpy.array(yTestTarget), argumentPercent, 2, 15, clusterNum)
                    algorithmsBestParamsDict[algorithm[2]]['sPT__percentile'] = AggloBestParams['agglo_percent']
                    algorithmsBestParamsDict[algorithm[2]]['agglo_knn'] = AggloBestParams['agglo_knn']
                    algorithmsBestParamsDict[algorithm[2]]['agglo_linkage'] = AggloBestParams['agglo_linkage']
                    algorithmsBestParamsDict[algorithm[2]]['agglo_score'] = AggloBestParams['agglo_score']
                    algorithmsBestParamsDict[algorithm[2]]['agglo_predictScore'] = AggloBestParams['agglo_predictScore']
                    dataHistogram[algorithm[2]+1][1] = algorithmsBestParamsDict[algorithm[2]]['agglo_score']
                    dataHistogram[algorithm[2]+1][2] = algorithmsBestParamsDict[algorithm[2]]['agglo_predictScore']
                    dataHistogram[algorithm[2]+1][4] = algorithmsBestParamsDict[algorithm[2]]['sPT__percentile']
                    # estimators = [('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()), ('to_dense', DenseTransformer()), (algorithm[0], algorithm[1])] #, ('kNC', KNeighborsClassifier())]
                    # knnList.append(None)
                    # params = dict(tfIdf__smooth_idf=[True],
                    #               sPT__score_func=[f_classif], sPT__percentile=range(10, 0, -1),
                    #               AgglomerativeTrans__linkage=['average', 'complete', 'ward'], AgglomerativeTrans__n_clusters=[clusterNum],
                    #               # AgglomerativeTrans__connectivity=[None])
                    #               AgglomerativeTrans__connectivity=[None])


                if 'AgglomerativeTrans' not in algorithm[0]:
                    pipe = Pipeline(estimators)
                    # v_measure_scorer = make_scorer(v_measure_score)
                    v_measure_scorer = make_scorer(v_measure_score)
                    grid_search = GridSearchCV(pipe, verbose=100, error_score=1, param_grid=params, cv=StratifiedKFold(n_splits=3), scoring=v_measure_scorer)
                    grid_search_fit = grid_search.fit(apiVectorArray, yTarget)

                    print("\n\ncv_results_=>\n" + str(grid_search_fit.cv_results_))
                    print("\nbest_estimator_=>\n" + str(grid_search_fit.best_estimator_))
                    print("\nbest_index_=>\n" + str(grid_search_fit.best_index_))
                    print("\nbest_params_=>\n" + str(grid_search_fit.best_params_))
                    print("\nbest_params_=> sPT__score_func\n" + str(grid_search_fit.best_params_['sPT__score_func']))
                    print("\nbest_params_=> sPT__percentile\n" + str(grid_search_fit.best_params_['sPT__percentile']))
                    print("\nbest_score_=>\n" + str(grid_search_fit.best_score_))
                    # print("\nscorer_=>\n" + str(grid_search_fit.scorer_))

                    print("\npredict=>")
                    # print("\nlabels=>" + str(len(grid_search_fit.predict(apiSequenceStringVectorTest))))
                    printNumPerLine(grid_search_fit.predict(apiVectorTestArray), (numberFromEachFamily - trainingNumber))
                    print("\ngrid_search_fit.best_estimator_.predict=> " + str(v_measure_score(yTestTarget, grid_search_fit.best_estimator_.predict(apiVectorTestArray))))

                    ##### Save data
                    #
                    algorithmsBestParamsDict[algorithm[2]]['sPT__percentile'] = grid_search_fit.best_params_['sPT__percentile']
                    # if 'MeanShift' in algorithm[0]:
                    #     algorithmsBestParamsDict[algorithm[2]]['sPT__percentile'] = grid_search_fit.best_params_['sPT__percentile']
                    # elif 'KMeans' in algorithm[0]:
                    #     algorithmsBestParamsDict[algorithm[2]]['sPT__percentile'] = grid_search_fit.best_params_['sPT__percentile']
                    if 'DBScan' in algorithm[0]:
                        algorithmsBestParamsDict[algorithm[2]]['DBScanTrans__n_neighbors'] = grid_search_fit.best_params_['DBScanTrans__n_neighbors']
                        algorithmsBestParamsDict[algorithm[2]]['DBScanTrans__eps'] = grid_search_fit.best_params_['DBScanTrans__eps']
                        algorithmsBestParamsDict[algorithm[2]]['DBScanTrans__min_samples'] = grid_search_fit.best_params_['DBScanTrans__min_samples']

                    hCVmeasure = homogeneity_completeness_v_measure(yTestTarget, grid_search_fit.best_estimator_.predict(apiVectorTestArray))
                    dataHistogram[algorithm[2]+1][1] = grid_search_fit.best_score_
                    dataHistogram[algorithm[2]+1][2] = hCVmeasure[2]
                    dataHistogram[algorithm[2]+1][3] = hCVmeasure[0]
                    dataHistogram[algorithm[2]+1][4] = hCVmeasure[1]
                    dataHistogram[algorithm[2]+1][6] = grid_search_fit.best_params_['sPT__percentile']

                    # hCVmeasure = homogeneity_completeness_v_measure(yTestTarget, grid_search_fit.best_estimator_.predict(apiVectorTestArray))
                    # print("hCVmeasure=>" + str(hCVmeasure))
                    # print("hCVmeasure homogeneity=>" + str(hCVmeasure[0]))
                    # print("hCVmeasure completeness=>" + str(hCVmeasure[1]))
                    # print("hCVmeasure v_measure=>" + str(hCVmeasure[2]))
                    #
                    ##### Save data


                    print("\nparams=>")
                    if 'MeanShift' in algorithm[0]:
                        params = dict(tfIdf__smooth_idf=True,
                                      sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'])
                    elif 'KMeans' in algorithm[0]:
                        params = dict(tfIdf__smooth_idf=True,
                                      sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'],
                                      KMeans__n_clusters=clusterNum, KMeans__random_state=0, KMeans__precompute_distances=True)
                    elif 'DBScan' in algorithm[0]:
                        params = dict(tfIdf__smooth_idf=True,
                                      sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'],
                                      DBScanTrans__eps=grid_search_fit.best_params_['DBScanTrans__eps'], DBScanTrans__min_samples=grid_search_fit.best_params_['DBScanTrans__min_samples'],
                                      DBScanTrans__n_neighbors=grid_search_fit.best_params_['DBScanTrans__n_neighbors'])
                    elif 'BIRCH' in algorithm[0]:
                        params = dict(tfIdf__smooth_idf=True,
                                      sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'],
                                      BIRCH__n_clusters=clusterNum)
                    # elif 'AgglomerativeTrans' in algorithm[0]:
                    #     params = dict(tfIdf__smooth_idf=True,
                    #                   sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'],
                    #                   AgglomerativeTrans__linkage=grid_search_fit.best_params_['AgglomerativeTrans__linkage'],
                    #                   AgglomerativeTrans__n_clusters=grid_search_fit.best_params_['AgglomerativeTrans__n_clusters'],
                    #                   AgglomerativeTrans__connectivity=grid_search_fit.best_params_['AgglomerativeTrans__connectivity'])
                        # AgglomerativeTrans__connectivity=[None, knnList])

                    print(params)
                    print("\npipe.set_params(params)=>" + str(v_measure_score(yTestTarget, pipe.set_params(**params).fit(apiVectorArray, yTarget).predict(apiVectorTestArray))))

                endTime = time.time()
                elapsedTime = endTime - startTime
                dataHistogram[algorithm[2]+1][5] = elapsedTime/60
                print("dataHistogram=>" + str(dataHistogram))
                print("algorithmsBestParamsDict=>" + str(algorithmsBestParamsDict))
                print("\n<<< API Vector " + str(algorithm[0]) + "    elapsed time=> " + str(elapsedTime/60) + " minutes <<<")
        # sys.exit("ERROR")
        print("==>Histogram<==")
        print(pd.DataFrame(data=dataHistogram[1:, 1:], index=dataHistogram[1:, 0], columns=dataHistogram[0, 1:]))

        with open("result-" + str(datetime.date.today()) + ".txt", "a") as resultFile:
            resultFile.write(">>> API VECTOR <<<\n")
            resultFile.write(str(algorithmsBestParamsDict))
            resultFile.write("\n\n")
            resultFile.write(str(pd.DataFrame(data=dataHistogram[1:, 1:], index=dataHistogram[1:, 0], columns=dataHistogram[0, 1:])))
            resultFile.write("\n\n\n")

        print("___ Histogram  numberFromEachFamily:" + str(numberFromEachFamily) + " ___")
    #
    ##########



    ##########
    # behavior -> processes -> process_path + calls ->

    # createSequenceList = ["time", "random", "normal"]
    # createSequenceList = ["normal", "argument", "time"]
    # createSequenceList = ["argument", "normal", "time"]  # "time" must be put in the end of the list
    createSequenceList = ["time"]  # "time" must be put in the end of the list

    #########>>> WINDOW TIME <<<#########
    #
    if arguments[2] == 'True':
        if "time" in createSequenceList:
            timePeriodList = [0.1, 0.4, 1]
            refinedList = [True]
            argumentPercentTime = 100

            createApiSequenceVectorFunc = createApiSequenceVectorByTimeWindow
            for toRefine in refinedList:
                for timePeriod in timePeriodList:
                    if toRefine:
                        apiSequenceVectorFile = "apiSequenceVectorTimeWindow_" + str(numberFromEachFamily) + "_" + str(timePeriod) + "_refined.txt"
                    else:
                        apiSequenceVectorFile = "apiSequenceVectorTimeWindow_" + str(numberFromEachFamily) + "_" + str(timePeriod) + ".txt"

                    if deleteApiList == 'y':
                        if not os.path.exists('apiList.txt'):
                            if numberFromEachFamily == 2:
                                copyfile("apiList_2.txt", "apiList.txt")
                            elif numberFromEachFamily == 5:
                                copyfile("apiList_5.txt", "apiList.txt")
                            elif numberFromEachFamily == 25:
                                copyfile("apiList_25.txt", "apiList.txt")
                            elif numberFromEachFamily == 50:
                                copyfile("apiList_50.txt", "apiList.txt")

                        apiSequenceVectorAll = createApiSequenceVectorFunc(familyDirectoryDict, timePeriod, toRefine)

                        with open(apiSequenceVectorFile, "wb") as aSV:
                            pickle.dump(apiSequenceVectorAll, aSV)
                    else:
                        with open(apiSequenceVectorFile, 'rb') as aSV:
                            apiSequenceVectorAll = pickle.load(aSV)

                    # print("apiSequenceVectorAll=>" + str(apiSequenceVectorAll))
                    # print("apiSequenceVectorAll[14] refined:" + str(toRefine) + " Time:" + str(timePeriod) + " => \n" + str(apiSequenceVectorAll[14]))
                    # print("apiVectorArray => \n" + str(apiVectorArray))
                    # print(apiSequenceVectorFile)
                    # print("apiSequenceVectorAll refined:" + str(toRefine) + " Time:" + str(timePeriod) + " shape:" + str(np.asarray(apiSequenceVectorAll).shape)
                    #       + " numberFromEachFamily:" + str(numberFromEachFamily))
                    # print("nonzero=>" + str(np.asarray(apiSequenceVectorAll[0]).nonzero()))
                    # transformer = TfidfTransformer(smooth_idf=True)
                    # print("apiSequenceVectorAll refined:" + str(toRefine) + " Time:" + str(timePeriod) + " TFIDF shape:" + str(transformer.fit_transform(apiSequenceVectorAll).toarray().shape)
                    #      + " numberFromEachFamily:" + str(numberFromEachFamily))

                    print(">>>   time   refined   period:" + str(timePeriod) + "   <<<")
                    transformer = TfidfTransformer(smooth_idf=True)
                    transformerFitTransform = transformer.fit_transform(apiSequenceVectorAll)
                    # str(transformer.fit_transform(apiSequenceVectorAll).toarray().shape)
                    print("Shape:" + str(transformerFitTransform.shape))
                    nonzeroCounter = 0
                    for i in xrange(transformerFitTransform.shape[0]):
                        print("transformerFitTransform[" + str(i) + "]=> nonzero:" + str(len(transformerFitTransform[i].nonzero()[0])))
                        nonzeroCounter += len(transformerFitTransform[i].nonzero()[0])
                        # print("tfidfVecTransform[" + str(i) + "]=> length:" + str(tfidfVecTransform.shape))
                    percent = float(nonzeroCounter) / float(transformerFitTransform.shape[0] * transformerFitTransform.shape[1])
                    # print("apiSequenceStringVector " + str(createSequenceBy) + " refined:" + str(refined) + " TFIDF shpape:" +
                    #       str(tfidfVec.fit_transform(apiSequenceStringVector).toarray().shape) +
                    #       " numberFromEachFamily:" + str(numberFromEachFamily))
                    print("nonzeroPercent=>" + str(percent))
                    print("_____________________________")


                    if False:
                        apiSequenceVector = list()
                        # print("apiSequenceVector=>" + str(apiSequenceVector))

                        selectList = selectListFunction(numberFromEachFamily, clusterNum, trainingNumber, "train")
                        for i in selectList:
                            # print("i=>" + str(i))
                            # print("apiSequenceVectorAll=>" + str(apiSequenceVectorAll[i]))
                            apiSequenceVector.append(apiSequenceVectorAll[i])
                            # print("apiSequenceVector=>" + str(apiSequenceVector))

                        apiSequenceVectorTest = list()
                        selectList = selectListFunction(numberFromEachFamily, clusterNum, trainingNumber, "test")
                        for i in selectList:
                            apiSequenceVectorTest.append(apiSequenceVectorAll[i])
                        backUpApiSequenceVectorTest = copy.deepcopy(apiSequenceVectorTest)
                        # apiSequenceVectorTestRefined = removeRepetitonAPISeq(apiSequenceVectorTest)

                        backUpApiSequenceVector = copy.deepcopy(apiSequenceVector)
                        # apiSequenceVectorRefined = removeRepetitonAPISeq(apiSequenceVector)

                        # print("apiSequenceVector=>" + str(apiSequenceVector))

                        print("\n############ START ############")
                        print("### API Sequence Evaluation ###")
                        print("### API Sequences ordered by TIME WINDOW:" + str(timePeriod) + " ! ###")
                        print("### FILE: " + apiSequenceVectorFile + " ! ###")

                        if toRefine:
                            print("\n>>>Refined (eliminate duplicate API subsequences)>>>")
                        else:
                            print("\n>>>Normal>>>")
                            # print("0 normal#@! " + str(apiSequenceVector[0]))

                        apiSequenceVector = backUpApiSequenceVector
                        apiSequenceVectorTest = backUpApiSequenceVectorTest

                        print("apiSequenceVector=>" + str(apiSequenceVector[0]))

                        for algorithm in clusterAlgorithm:
                            startTime = time.time()
                            print("\n>>>API SEQUENCE " + str(algorithm[0]) + " >>>")
                            if 'MeanShift' in algorithm[0]:
                                estimators = [('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()),
                                              ('to_dense', DenseTransformer()), (algorithm[0], algorithm[1])]
                                params = dict(tfIdf__smooth_idf=[True],
                                              sPT__score_func=[f_classif], sPT__percentile=range(argumentPercentTime, 0, -1))
                            elif 'KMeans' in algorithm[0]:
                                estimators = [('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()),
                                              (algorithm[0], algorithm[1])]  # ('lDA', LatentDirichletAllocation())
                                params = dict(tfIdf__smooth_idf=[True],
                                              # sPT__score_func= [chi2, f_classif, mutual_info_classif], sPT__percentile=range(100, 0, -1),
                                              sPT__score_func=[f_classif], sPT__percentile=range(argumentPercentTime, 0, -1),
                                              # lDA__n_topics=[clusterNum], lDA__max_iter=[10], lDA__learning_method=['online'], lDA__learning_offset=[50.], lDA_random_state=[0])
                                              KMeans__n_clusters=[clusterNum], KMeans__random_state=[0],
                                              KMeans__precompute_distances=[True])
                            elif 'BIRCH' in algorithm[0]:
                                estimators = [('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()),
                                              (algorithm[0], algorithm[1])]
                                params = dict(tfIdf__smooth_idf=[True],
                                              sPT__score_func=[f_classif], sPT__percentile=range(argumentPercentTime, 0, -1),
                                              BIRCH__n_clusters=[clusterNum])

                            if 'AgglomerativeTrans' not in algorithm[0]:
                                 # estimators = [('tfIdfVectorizer', TfidfVectorizer()), ('sPT', SelectPercentile()), ('kmeans', cluster.KMeans())]
                                # estimators = [('countVec', CountVectorizer()), ('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()),
                                #               ('kmeans', cluster.KMeans())]
                                pipe = Pipeline(estimators)
                                # params = dict(tfIdfVectorizer__ngram_range=[(2, 2), (3, 3), (4, 4), (5, 5)], tfIdfVectorizer__token_pattern=[r'\b\w+\b'],
                                #               tfIdfVectorizer__min_df=[1], tfIdfVectorizer__smooth_idf=[True],
                                #               # countVec__ngram_range=[(2, 2), (3, 3), (4, 4), (5, 5)], countVec__token_pattern=[r'\b\w+\b'], countVec__min_df=[1],
                                #               #countVec__ngram_range=[(4, 4)], countVec__token_pattern=[r'\b\w+\b'], countVec__min_df=[1],
                                #               # tfIdf__smooth_idf=[True],
                                #               #sPT__score_func=[chi2, f_classif, mutual_info_classif], sPT__percentile=range(100, 0, -1),
                                #               sPT__score_func=[f_classif], sPT__percentile=range(100, 0, -1),
                                #               kmeans__n_clusters=[clusterNum], kmeans__random_state=[0], kmeans__precompute_distances=[True])
                                v_measure_scorer = make_scorer(v_measure_score)
                                # grid_search = GridSearchCV(pipe, verbose=100, error_score=1, param_grid=params, cv=StratifiedKFold(n_splits=3), scoring=v_measure_scorer)
                                grid_search = GridSearchCV(pipe, verbose=100, param_grid=params, cv=StratifiedKFold(n_splits=3),
                                                           scoring=v_measure_scorer)
                                print("grid_search =>" + str(grid_search))
                                grid_search_fit = grid_search.fit(apiSequenceVector, yTarget)

                                print("\n\ncv_results_=>\n" + str(grid_search_fit.cv_results_))
                                print("\nbest_estimator_=>\n" + str(grid_search_fit.best_estimator_))
                                print("\nbest_index_=>\n" + str(grid_search_fit.best_index_))
                                print("\nbest_params_=>\n" + str(grid_search_fit.best_params_))
                                print("\nbest_params_=> sPT__score_func\n" + str(grid_search_fit.best_params_['sPT__score_func']))
                                print("\nbest_params_=> sPT__percentile\n" + str(grid_search_fit.best_params_['sPT__percentile']))
                                print("\nbest_score_=>\n" + str(grid_search_fit.best_score_))

                                print("\npredict=>")
                                printNumPerLine(grid_search_fit.predict(apiSequenceVectorTest),
                                                (numberFromEachFamily - trainingNumber))
                                print("\ngrid_search_fit.best_estimator_.predict=> " +
                                      str(v_measure_score(yTestTarget, grid_search_fit.best_estimator_.predict(
                                          apiSequenceVectorTest))))

                                ##### Save data
                                #
                                algorithmsBestParamsSequenceDict[algorithm[2]]['sPT__percentile'] = grid_search_fit.best_params_['sPT__percentile']

                                hCVmeasure = homogeneity_completeness_v_measure(yTestTarget,
                                                                        grid_search_fit.best_estimator_.predict(apiSequenceVectorTest))
                                dataSequence[algorithm[2] + 1][1] = grid_search_fit.best_score_
                                dataSequence[algorithm[2] + 1][2] = hCVmeasure[2]
                                dataSequence[algorithm[2] + 1][3] = hCVmeasure[0]
                                dataSequence[algorithm[2] + 1][4] = hCVmeasure[1]
                                dataSequence[algorithm[2] + 1][6] = grid_search_fit.best_params_['sPT__percentile']
                                dataSequence[algorithm[2] + 1][7] = timePeriod
                                #
                                ##### Save data

                            endTime = time.time()
                            elapsedTime = endTime - startTime
                            dataSequence[algorithm[2] + 1][5] = elapsedTime / 60

                    print("algorithmsBestParamsDict=>" + str(algorithmsBestParamsDict))
                    print("==>Sequence<==")
                    print(pd.DataFrame(data=dataSequence[1:, 1:], index=dataSequence[1:, 0], columns=dataSequence[0, 1:]))
                    print("___Sequence___")
                    print("\n<<<End refined (" + str(toRefine) + ") TIME:" + str(timePeriod) + " numberFromEachFamily:" + str(numberFromEachFamily) + " <<<\n")
    #
    #########___ WINDOW TIME ___#########


    numpy.set_printoptions(threshold=numpy.nan)
    numpy.set_printoptions(threshold='nan')
    # nGramList = [(2, 2), (3, 3), (4, 4)]
    nGramList = [(2, 2)]
    for nGram in nGramList:
        for createSequenceBy in createSequenceList:
            if "normal" in createSequenceBy or "random" in createSequenceBy:
                createApiSequenceVectorFunc = createApiSequenceVector
                apiSequenceVectorFile = "apiSequenceVector_" + str(numberFromEachFamily) + ".txt"
            # elif "time" in createSequenceBy :
            #     createApiSequenceVectorFunc = createApiSequenceVectorByTimeWindow
            #     apiSequenceVectorFile = "apiSequenceVectorTimeWindow_" + str(numberFromEachFamily) + ".txt"
            elif "argument" in createSequenceBy :
                createApiSequenceVectorFunc = createApiSequenceVectorWithArgumentValue
                apiSequenceVectorFile = "apiSequenceVectorArgValue_" + str(numberFromEachFamily) + ".txt"
            else:
                break
            print (apiSequenceVectorFile)
            # elif "random" in createSequenceBy:
            #     createApiSequenceVectorFunc = createApiSequenceVector
            #     apiSequenceVectorFile = "apiSequenceVector.txt"

            if arguments[2]=='True':
                if deleteApiList == 'y':
                    if not os.path.exists('apiList.txt'):
                        if numberFromEachFamily == 25:
                            copyfile("apiList_25.txt", "apiList.txt")
                        elif numberFromEachFamily == 50:
                            copyfile("apiList_50.txt", "apiList.txt")

                    apiSequenceVectorAll = createApiSequenceVectorFunc(familyDirectoryDict)

                    with open(apiSequenceVectorFile, "wb") as aSV:
                        pickle.dump(apiSequenceVectorAll, aSV)
                else:
                    with open(apiSequenceVectorFile, 'rb') as aSV:
                        apiSequenceVectorAll = pickle.load(aSV)
                # sys.exit("ERROR")

                # print("before random=>" + str(apiSequenceVectorAll))
                if "random" in createSequenceBy:
                    for i in xrange(len(apiSequenceVectorAll)):
                        shuffle(apiSequenceVectorAll[i])
                # print("after random=>" + str(apiSequenceVectorAll))

                #####>>> apiSequenceVector >>>
                #
                apiSequenceVector = list()
                selectList = selectListFunction(numberFromEachFamily, clusterNum, trainingNumber, "train")
                for i in selectList:
                    apiSequenceVector.append(apiSequenceVectorAll[i])
                # print("\napiSequenceVector => ")
                # print(apiSequenceVector)
                #
                #####<<< apiSequenceVector <<<

                #####>>> apiSequenceVectorTest >>>
                #
                apiSequenceVectorTest = list()
                selectList = selectListFunction(numberFromEachFamily, clusterNum, trainingNumber, "test")
                for i in selectList:
                    apiSequenceVectorTest.append(apiSequenceVectorAll[i])
                backUpApiSequenceVectorTest = copy.deepcopy(apiSequenceVectorTest)
                apiSequenceVectorTestRefined = removeRepetitonAPISeq(apiSequenceVectorTest)
                # apiSequenceStringVectorTest = vectorToStringVector(apiSequenceVectorTest)
                # print("\napiSequenceStringVectorTest=>" + str(apiSequenceStringVectorTest))
                #
                #####<<< apiSequenceVectorTest <<<

                # >>> http://www.python-course.eu/passing_arguments.php >>>
                # http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#other-languages-have-variables
                # If you pass immutable arguments like integers, strings or tuples to a function,
                # the passing acts like call-by-value. The object reference is passed to the function parameters. They can't be changed within the function.
                # If we pass mutable arguments. They are also passed by object reference, but they can be changed in place in the function.
                #
                # Python always passes mutable arguments BY-OBJECT-REFERENCE. Copy first before calling removeRepetitionAPISeq.
                # <<< By passing a copy to the function, a shallow copy is sufficient: func2(fib[:]) or func3([x[:] for x in apiSequenceVector]) <<<
                backUpApiSequenceVector = copy.deepcopy(apiSequenceVector)
                # sys.exit("Error message")

                # Remove repeated api subsequences
                apiSequenceVectorRefined = removeRepetitonAPISeq(apiSequenceVector)
                # print("\nremoveRepetitonAPISeq => ")
                # print(str(apiSequenceVectorRefined))

                # refinedList = [True, False]
                refinedList = [True]
                print("\n############ START ############")
                print("### API Sequence Evaluation ###")
                if "time" in createSequenceBy:
                    print("### API Sequences ordered by TIME! ###")
                elif "argument" in createSequenceBy:
                    print("### API Sequences plus ARGUMENTS! ###")
                elif "random" in createSequenceBy:
                    print("### API Sequences ordered RANDOMLY! ###")

                for refined in refinedList:
                    if refined:
                        print("\n>>>Refined (eliminate duplicate API subsequences)>>>")
                        apiSequenceVector = apiSequenceVectorRefined
                        apiSequenceVectorTest = apiSequenceVectorTestRefined
                        #print("0 refined#@! " + str(apiSequenceVector[0]))
                    else:
                        print("\n>>>Normal>>>")
                        apiSequenceVector = backUpApiSequenceVector
                        apiSequenceVectorTest = backUpApiSequenceVectorTest
                        #print("0 normal#@! " + str(apiSequenceVector[0]))


                    # Generate StringVectors
                    apiSequenceStringVector = vectorToStringVector(apiSequenceVector)
                    apiSequenceStringVectorTest = vectorToStringVector(apiSequenceVectorTest)

                    print("apiSequenceVector=>" + str(apiSequenceVector[0]))
                    print("apiSequenceVector 1=>" + str(apiSequenceVector[0+numberFromEachFamily]))
                    # print("apiSequenceStringVector " + str(createSequenceBy) + " refined:" + str(refined) + " => \n" + str(apiSequenceStringVector))
                    # print("apiSequenceStringVector " + str(createSequenceBy) + " refined:" + str(refined) + " len: => " + str(len(apiSequenceStringVector[0])))

                    # for ngram in xrange(2, 5):
                    # numpy.set_printoptions(threshold=1e6)
                    numpy.set_printoptions(threshold=numpy.nan)
                    # numpy.set_printoptions(threshold='nan')
                    for ngram in xrange(2, 5):
                        # vectorizer = CountVectorizer(min_df=1, token_pattern=r'\b\w+\b', ngram_range=(ngram, ngram))
                        # vectorizedStringVector = vectorizer.fit_transform(apiSequenceStringVector)
                        # print("apiSequenceStringVector " + str(createSequenceBy) + " refined:" + str(refined) + " shape:" + str(vectorizedStringVector.shape)
                        #       + " ngram:" + str(ngram) + " numberFromEachFamily:" + str(numberFromEachFamily))
                        #
                        # print("vectorizer.fit_transform(apiSequenceStringVector)=>")
                        # print(vectorizedStringVector[0,:])
                        # print("### nonzero=>" + str(vectorizedStringVector[0].nonzero()))

                        tfidfVec = TfidfVectorizer(smooth_idf=True, min_df=1, token_pattern=r'\b\w+\b', ngram_range=(ngram, ngram))
                        tfidfVecTransform = tfidfVec.fit_transform(apiSequenceStringVector)
                        print(">>>   " + str(createSequenceBy) + "   refined:" + str(refined) + "   ngram:" + str(ngram) + "   <<<")
                        print("Shape:" + str(tfidfVecTransform.shape))
                        nonzeroCounter = 0
                        for i in xrange(tfidfVecTransform.shape[0]):
                            print("tfidfVecTransform[" + str(i) + "]=> nonzero:" + str(len(tfidfVecTransform[i].nonzero()[0])))
                            nonzeroCounter += len(tfidfVecTransform[i].nonzero()[0])
                            # print("tfidfVecTransform[" + str(i) + "]=> length:" + str(tfidfVecTransform.shape))
                        percent = float(nonzeroCounter) / float(tfidfVecTransform.shape[0]*tfidfVecTransform.shape[1])
                        # print("apiSequenceStringVector " + str(createSequenceBy) + " refined:" + str(refined) + " TFIDF shpape:" +
                        #       str(tfidfVec.fit_transform(apiSequenceStringVector).toarray().shape) +
                        #       " numberFromEachFamily:" + str(numberFromEachFamily))
                        print("nonzeroPercent=>" + str(percent))
                        print("_____________________________")

                    if False:
                        for algorithm in clusterAlgorithm:
                            startTime = time.time()
                            print("\n>>>API SEQUENCE " + str(algorithm[0]) + " >>>")
                            if 'KMeans' in algorithm[0]:
                                estimators = [('tfIdfVectorizer', TfidfVectorizer()), ('sPT', SelectPercentile()), (algorithm[0], algorithm[1])]  # ('lDA', LatentDirichletAllocation())
                                params = dict(tfIdfVectorizer__ngram_range=[nGram], tfIdfVectorizer__token_pattern=[r'\b\w+\b'],
                                              tfIdfVectorizer__min_df=[1], tfIdfVectorizer__smooth_idf=[True],
                                              # sPT__score_func= [chi2, f_classif, mutual_info_classif], sPT__percentile=range(100, 0, -1),
                                              sPT__score_func=[f_classif], sPT__percentile=range(argumentPercent, 0, -1),
                                              # lDA__n_topics=[clusterNum], lDA__max_iter=[10], lDA__learning_method=['online'], lDA__learning_offset=[50.], lDA_random_state=[0])
                                              KMeans__n_clusters=[clusterNum], KMeans__random_state=[0], KMeans__precompute_distances=[True])
                            elif 'MeanShift' in algorithm[0]:
                                estimators = [('tfIdfVectorizer', TfidfVectorizer()), ('sPT', SelectPercentile()),
                                              ('to_dense', DenseTransformer()), (algorithm[0], algorithm[1])]
                                params = dict(tfIdfVectorizer__ngram_range=[nGram], tfIdfVectorizer__token_pattern=[r'\b\w+\b'],
                                              tfIdfVectorizer__min_df=[1], tfIdfVectorizer__smooth_idf=[True],
                                              sPT__score_func=[f_classif], sPT__percentile=range(argumentPercent, 0, -1))
                            elif 'BIRCH' in algorithm[0]:
                                estimators = [('tfIdfVectorizer', TfidfVectorizer()), ('sPT', SelectPercentile()), (algorithm[0], algorithm[1])]
                                params = dict(tfIdfVectorizer__ngram_range=[nGram], tfIdfVectorizer__token_pattern=[r'\b\w+\b'],
                                              tfIdfVectorizer__min_df=[1], tfIdfVectorizer__smooth_idf=[True],
                                              sPT__score_func=[f_classif], sPT__percentile=range(argumentPercent, 0, -1),
                                              BIRCH__n_clusters=[clusterNum])
                            elif 'DBScanTrans' in algorithm[0]:
                                estimators = [('tfIdfVectorizer', TfidfVectorizer()), ('sPT', SelectPercentile()), (algorithm[0], algorithm[1])]  # , ('kNC', KNeighborsClassifier())]
                                params = dict(tfIdfVectorizer__ngram_range=[(2, 2), (3, 3), (4, 4)], tfIdfVectorizer__token_pattern=[r'\b\w+\b'],
                                              tfIdfVectorizer__min_df=[1], tfIdfVectorizer__smooth_idf=[True],
                                              sPT__score_func=[f_classif], sPT__percentile=range(argumentPercent, 0, -1),
                                              DBScanTrans__eps=np.arange(0.1, 2.8, 0.3), DBScanTrans__min_samples=range(5, 8),
                                              DBScanTrans__n_neighbors=[5])
                                              # DBScanTrans__eps=np.arange(0.1, 10.1, 0.1), DBScanTrans__min_samples=range(1, 11),
                                              # DBScanTrans__n_neighbors=[3, 4, 5])
                            elif 'AgglomerativeTrans' in algorithm[0]:
                                bestParamsPlusNGram = AgglomerativeGridSearchCVPlusCountVec(apiSequenceStringVector, numpy.array(yTarget), apiSequenceStringVectorTest, numpy.array(yTestTarget), argumentPercent, 2, 10, clusterNum)
                                print("\n\nAll BEST percent=>" + str(bestParamsPlusNGram['agglo_percent']))
                                print("All BEST knn=>" + str(bestParamsPlusNGram['agglo_knn']))
                                print("All BEST linkage=>" + str(bestParamsPlusNGram['agglo_linkage']))
                                print("All BEST score=>" + str(bestParamsPlusNGram['agglo_score']))
                                print("All BEST test score=>" + str(bestParamsPlusNGram['agglo_predictScore']))
                                print("All BEST N-Gram=>" + str(bestParamsPlusNGram['agglo_ngram']))
                                algorithmsBestParamsSequenceDict[algorithm[2]]['sPT__percentile'] = bestParamsPlusNGram['agglo_percent']
                                algorithmsBestParamsSequenceDict[algorithm[2]]['agglo_knn'] = bestParamsPlusNGram['agglo_knn']
                                algorithmsBestParamsSequenceDict[algorithm[2]]['agglo_linkage'] = bestParamsPlusNGram['agglo_linkage']
                                algorithmsBestParamsSequenceDict[algorithm[2]]['agglo_score'] = bestParamsPlusNGram['agglo_score']
                                algorithmsBestParamsSequenceDict[algorithm[2]]['agglo_predictScore'] = bestParamsPlusNGram['agglo_predictScore']
                                algorithmsBestParamsSequenceDict[algorithm[2]]['agglo_ngram'] = bestParamsPlusNGram['agglo_ngram']
                                dataSequence[algorithm[2] + 1][1] = algorithmsBestParamsSequenceDict[algorithm[2]]['agglo_score']
                                dataSequence[algorithm[2] + 1][2] = algorithmsBestParamsSequenceDict[algorithm[2]]['agglo_predictScore']
                                dataSequence[algorithm[2] + 1][4] = algorithmsBestParamsSequenceDict[algorithm[2]]['sPT__percentile']
                                dataSequence[algorithm[2] + 1][5] = algorithmsBestParamsSequenceDict[algorithm[2]]['agglo_ngram']


                            if 'AgglomerativeTrans' not in algorithm[0]:
                                # estimators = [('tfIdfVectorizer', TfidfVectorizer()), ('sPT', SelectPercentile()), ('kmeans', cluster.KMeans())]
                                # estimators = [('countVec', CountVectorizer()), ('tfIdf', TfidfTransformer()), ('sPT', SelectPercentile()),
                                #               ('kmeans', cluster.KMeans())]
                                pipe = Pipeline(estimators)
                                # params = dict(tfIdfVectorizer__ngram_range=[(2, 2), (3, 3), (4, 4), (5, 5)], tfIdfVectorizer__token_pattern=[r'\b\w+\b'],
                                #               tfIdfVectorizer__min_df=[1], tfIdfVectorizer__smooth_idf=[True],
                                #               # countVec__ngram_range=[(2, 2), (3, 3), (4, 4), (5, 5)], countVec__token_pattern=[r'\b\w+\b'], countVec__min_df=[1],
                                #               #countVec__ngram_range=[(4, 4)], countVec__token_pattern=[r'\b\w+\b'], countVec__min_df=[1],
                                #               # tfIdf__smooth_idf=[True],
                                #               #sPT__score_func=[chi2, f_classif, mutual_info_classif], sPT__percentile=range(100, 0, -1),
                                #               sPT__score_func=[f_classif], sPT__percentile=range(100, 0, -1),
                                #               kmeans__n_clusters=[clusterNum], kmeans__random_state=[0], kmeans__precompute_distances=[True])
                                v_measure_scorer = make_scorer(v_measure_score)
                                # grid_search = GridSearchCV(pipe, verbose=100, error_score=1, param_grid=params, cv=StratifiedKFold(n_splits=3), scoring=v_measure_scorer)
                                grid_search = GridSearchCV(pipe, verbose=100, param_grid=params, cv=StratifiedKFold(n_splits=3), scoring=v_measure_scorer)
                                print("grid_search =>" + str(grid_search))
                                grid_search_fit = grid_search.fit(apiSequenceStringVector, yTarget)

                                print("\n\ncv_results_=>\n" + str(grid_search_fit.cv_results_))
                                print("\nbest_estimator_=>\n" + str(grid_search_fit.best_estimator_))
                                print("\nbest_index_=>\n" + str(grid_search_fit.best_index_))
                                print("\nbest_params_=>\n" + str(grid_search_fit.best_params_))
                                print("\nbest_params_=> sPT__score_func\n" + str(grid_search_fit.best_params_['sPT__score_func']))
                                print("\nbest_params_=> sPT__percentile\n" + str(grid_search_fit.best_params_['sPT__percentile']))
                                print("\nbest_score_=>\n" + str(grid_search_fit.best_score_))

                                print("\npredict=>")
                                printNumPerLine(grid_search_fit.predict(apiSequenceStringVectorTest), (numberFromEachFamily - trainingNumber))
                                print("\ngrid_search_fit.best_estimator_.predict=> " +
                                      str(v_measure_score(yTestTarget, grid_search_fit.best_estimator_.predict(apiSequenceStringVectorTest))))

                                ##### Save data
                                #
                                algorithmsBestParamsSequenceDict[algorithm[2]]['sPT__percentile'] = grid_search_fit.best_params_['sPT__percentile']
                                # if 'MeanShift' in algorithm[0]:
                                #     algorithmsBestParamsDict[algorithm[2]]['sPT__percentile'] = grid_search_fit.best_params_['sPT__percentile']
                                # elif 'KMeans' in algorithm[0]:
                                #     algorithmsBestParamsDict[algorithm[2]]['sPT__percentile'] = grid_search_fit.best_params_['sPT__percentile']
                                if 'DBScan' in algorithm[0]:
                                    algorithmsBestParamsSequenceDict[algorithm[2]]['DBScanTrans__n_neighbors'] = grid_search_fit.best_params_['DBScanTrans__n_neighbors']
                                    algorithmsBestParamsSequenceDict[algorithm[2]]['DBScanTrans__eps'] = grid_search_fit.best_params_['DBScanTrans__eps']
                                    algorithmsBestParamsSequenceDict[algorithm[2]]['DBScanTrans__min_samples'] = grid_search_fit.best_params_['DBScanTrans__min_samples']

                                hCVmeasure = homogeneity_completeness_v_measure(yTestTarget, grid_search_fit.best_estimator_.predict(apiSequenceStringVectorTest))
                                dataSequence[algorithm[2] + 1][1] = grid_search_fit.best_score_
                                dataSequence[algorithm[2] + 1][2] = hCVmeasure[2]
                                dataSequence[algorithm[2] + 1][3] = hCVmeasure[0]
                                dataSequence[algorithm[2] + 1][4] = hCVmeasure[1]
                                dataSequence[algorithm[2] + 1][6] = grid_search_fit.best_params_['sPT__percentile']
                                dataSequence[algorithm[2] + 1][7] = str(grid_search_fit.best_params_['tfIdfVectorizer__ngram_range'])
                                #
                                ##### Save data

                                print("\nparams=>")
                                if 'MeanShift' in algorithm[0]:
                                    params = dict(tfIdfVectorizer__ngram_range=grid_search_fit.best_params_['tfIdfVectorizer__ngram_range'], tfIdfVectorizer__token_pattern=r'\b\w+\b',
                                                  tfIdfVectorizer__min_df=1, tfIdfVectorizer__smooth_idf=True,
                                                  sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'])
                                elif 'KMeans' in algorithm[0]:
                                    params = dict(tfIdfVectorizer__ngram_range=grid_search_fit.best_params_['tfIdfVectorizer__ngram_range'], tfIdfVectorizer__token_pattern=r'\b\w+\b',
                                                  tfIdfVectorizer__min_df=1, tfIdfVectorizer__smooth_idf=True,
                                                  sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'],
                                                  KMeans__n_clusters=clusterNum, KMeans__random_state=0, KMeans__precompute_distances=True)
                                elif 'DBScan' in algorithm[0]:
                                    params = dict(tfIdfVectorizer__ngram_range=grid_search_fit.best_params_['tfIdfVectorizer__ngram_range'], tfIdfVectorizer__token_pattern=r'\b\w+\b',
                                                  tfIdfVectorizer__min_df=1, tfIdfVectorizer__smooth_idf=True,
                                                  sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'],
                                                  DBScanTrans__eps=grid_search_fit.best_params_['DBScanTrans__eps'],
                                                  DBScanTrans__min_samples=grid_search_fit.best_params_['DBScanTrans__min_samples'],
                                                  DBScanTrans__n_neighbors=grid_search_fit.best_params_['DBScanTrans__n_neighbors'])
                                elif 'BIRCH' in algorithm[0]:
                                    params = dict(tfIdfVectorizer__ngram_range=grid_search_fit.best_params_['tfIdfVectorizer__ngram_range'], tfIdfVectorizer__token_pattern=r'\b\w+\b',
                                                  tfIdfVectorizer__min_df=1, tfIdfVectorizer__smooth_idf=True,
                                                  sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'],
                                                  BIRCH__n_clusters=clusterNum)

                                # params = dict(tfIdfVectorizer__ngram_range=grid_search_fit.best_params_['tfIdfVectorizer__ngram_range'], tfIdfVectorizer__token_pattern=r'\b\w+\b',
                                #               tfIdfVectorizer__min_df=1, tfIdfVectorizer__smooth_idf=True,
                                #               #countVec__ngram_range=grid_search_fit.best_params_['countVec__ngram_range'], countVec__token_pattern=r'\b\w+\b', countVec__min_df=1,
                                #               #countVec__ngram_range=[(4, 4)], countVec__token_pattern=[r'\b\w+\b'], countVec__min_df=[1],
                                #               # tfIdf__smooth_idf=True,
                                #               #sPT__score_func=[chi2, f_classif, mutual_info_classif], sPT__percentile=range(100, 0, -1),
                                #               sPT__score_func=f_classif, sPT__percentile=grid_search_fit.best_params_['sPT__percentile'],
                                #               kmeans__n_clusters=clusterNum, kmeans__random_state=0, kmeans__precompute_distances=True)
                                print(params)
                                print("\npipe.set_params(params)=>" + str(v_measure_score(yTestTarget,
                                                                                          pipe.set_params(**params).fit(
                                                                                              apiSequenceStringVector, yTarget).predict(apiSequenceStringVectorTest))))

                            endTime = time.time()
                            elapsedTime = endTime - startTime
                            dataSequence[algorithm[2] + 1][5] = elapsedTime/60
                            print("dataSequence=>" + str(dataSequence))
                            print("algorithmsBestParamsSequenceDict=>" + str(algorithmsBestParamsSequenceDict))
                            print("\n<<<API SEQUENCE " + str(algorithm[0]) + "   elapsed time=>" + str(elapsedTime/60) + " minutes   refined(" + str(refined) + ")<<<")

                        print("==>Sequence<==")
                        print(pd.DataFrame(data=dataSequence[1:, 1:], index=dataSequence[1:, 0], columns=dataSequence[0, 1:]))

                        with open("result-" + str(datetime.date.today()) + ".txt", "a") as resultFile:
                            if refined:
                                resultFile.write(">>> API SEQUENCE REFINED " + createSequenceBy + "<<<\n")
                            else:
                                resultFile.write(">>> API SEQUENCE NORMAL " + createSequenceBy + "<<<\n")
                            resultFile.write(str(algorithmsBestParamsSequenceDict))
                            resultFile.write("\n\n")
                            resultFile.write(str(pd.DataFrame(data=dataSequence[1:, 1:], index=dataSequence[1:, 0], columns=dataSequence[0, 1:])))
                            resultFile.write("\n\n\n")

                        print("___Sequence___")
                        print("\n<<<End refined (" + str(refined) + ") " + createSequenceBy + " numberFromEachFamily:" + str(numberFromEachFamily) + "<<<\n")
                print("### API Sequence Evaluation ###")
                print("############ FINISH ############\n")

    sys.exit("ERROR")
    #
    ##########

    ##########
    # Test
    # X = np.reshape(apiVector, (1, -1))
    # scaler = preprocessing.StandardScaler().fit(X_train)
    # X_train_transformed = scaler.transform(X_train)
    # clf = svm.SVC(C=1).fit(X_train_transformed, y_train)
    # X_test_transformed = scaler.transform(X_test)
    #
    ##########


    # print("apiVectorSparse[0] => " + str(apiVectorSparse[0]))
    # print("apiVectorSparse[1] => " + str(apiVectorSparse[1]))
    # print("apiVectorSparse[20] => " + str(apiVectorSparse[20]))
    # print("apiVectorSparse[21] => " + str(apiVectorSparse[21]))
    #apiVectorSparseScaler = preprocessing.StandardScaler(with_mean=False).fit_transform(apiVectorSparse)
    #print("\napiVectorSparseScaler => \n" + str(apiVectorSparseScaler))

    # Generate a test vector
    # print("\ntestApiVectorSparse[0] => " + str(testApiVectorSparse[0]))
    # print("\ntestApiVectorSparse[1] => " + str(testApiVectorSparse[1]))
    # print("\ntestApiVectorSparse[2] => " + str(testApiVectorSparse[2]))
    # print("\ntestApiVectorSparse[3] => " + str(testApiVectorSparse[3]))
    # sys.exit("ERROR")

    # pca = decomposition.PCA(n_components=2)
    #print("kmeans center => " + str(k_means.cluster_centers_))
    # print("\nkmenas predict (Sparse) => ")
    # printSeparate(k_means.predict(testApiVectorSparse), 2)

    ##########
    # Handle different algorithms
    # Use sparse
    apiVectorSparse = sp.csr_matrix(apiVector)
    # print("apiVectorSparse => " + str(apiVectorSparse))
    # print("apiVectorSparse.shape => " + str(apiVectorSparse.shape))

    # Use sparse and normalized
    apiVectorSparseNormalized = preprocessing.normalize(apiVectorSparse, norm='l2')

    # Use sparse and TFIDF
    transformer = TfidfTransformer(smooth_idf=True)
    apiVectorSparseTFIDF = transformer.fit_transform(apiVectorSparse)
    #print("TFIDF => " + str(tfidf.toarray()))

    # Use sparse and standardScaler
    apiVectorSparseScaler = preprocessing.StandardScaler(with_mean=False).fit_transform(apiVectorSparse)
    #print("StandardScaler => " + str(scalerApiVector))

    # Use RobustScaler (cannot employ sparse)
    apiVectorOutlier = preprocessing.RobustScaler().fit_transform(apiVector)
    #
    ##########


    ##########
    # Put a vector space into KMeans and generate a test vector space to evaluate the result
    print("\n############# START #############")
    print("### API Statistics Evaluation ###")

    testApiVectorSparse = getTestVector(apiVectorSparse, 2, len(malwareFamilies))
    # print("\ntestApiVectorSparse => " + str(testApiVectorSparse))
    # print("\ntestApiVectorSparse.shape => " + str(testApiVectorSparse.shape))
    k_means = cluster.KMeans(n_clusters=clusterNum, random_state=0, precompute_distances=True).fit(apiVectorSparse[separate:])
    #print("apiVector[:6] => " + str(apiVector[:6]))
    #print("\nkmeans label (Sparse) => " + str(k_means.labels_))
    print("\nkmeans label (Sparse) => ")
    printSeparate(k_means.labels_, k_means.predict(testApiVectorSparse), numberFromEachFamily, 2)

    k_means = cluster.KMeans(n_clusters=clusterNum, random_state=0, precompute_distances=True).fit(apiVectorSparseNormalized[separate:])
    print("\nkmeans label (sparse normalized) => ")
    testApiVectorSparse = getTestVector(apiVectorSparseNormalized, 2, len(malwareFamilies))
    # print("kmenas predict (sparse tfidf) => " + str(k_means.predict(testApiVectorSparse)) + "\n")
    printSeparate(k_means.labels_, k_means.predict(testApiVectorSparse), numberFromEachFamily, 2)
    # print("apiVectorSparseNormalized[0] => " + str(apiVectorSparseNormalized[0]))
    # print("apiVectorSparseNormalized[1] => " + str(apiVectorSparseNormalized[1]))
    # print("apiVectorSparseNormalized[20] => " + str(apiVectorSparseNormalized[20]))
    # print("apiVectorSparseNormalized[21] => " + str(apiVectorSparseNormalized[21]))
    # print("\ntestApiVectorSparse[0] => " + str(testApiVectorSparse[0]))
    # print("\ntestApiVectorSparse[1] => " + str(testApiVectorSparse[1]))
    # print("\ntestApiVectorSparse[2] => " + str(testApiVectorSparse[2]))
    # print("\ntestApiVectorSparse[3] => " + str(testApiVectorSparse[3]))

    k_means = cluster.KMeans(n_clusters=clusterNum, random_state=0, precompute_distances=True).fit(apiVectorSparseTFIDF[separate:])
    print("\nkmeans label (sparse tfidf) => ")
    testApiVectorSparse = getTestVector(apiVectorSparseTFIDF, 2, len(malwareFamilies))
    # print("kmenas predict (sparse tfidf) => " + str(k_means.predict(testApiVectorSparse)) + "\n")
    printSeparate(k_means.labels_, k_means.predict(testApiVectorSparse), numberFromEachFamily, 2)
    # print("apiVectorSparseTFIDF[0] => " + str(apiVectorSparseTFIDF[0]))
    # print("apiVectorSparseTFIDF[1] => " + str(apiVectorSparseTFIDF[1]))
    # print("apiVectorSparseTFIDF[20] => " + str(apiVectorSparseTFIDF[20]))
    # print("apiVectorSparseTFIDF[21] => " + str(apiVectorSparseTFIDF[21]))
    # print("\ntestApiVectorSparse[0] => " + str(testApiVectorSparse[0]))
    # print("\ntestApiVectorSparse[1] => " + str(testApiVectorSparse[1]))
    # print("\ntestApiVectorSparse[2] => " + str(testApiVectorSparse[2]))
    # print("\ntestApiVectorSparse[3] => " + str(testApiVectorSparse[3]))

    k_means = cluster.KMeans(n_clusters=clusterNum, random_state=0, precompute_distances=True).fit(apiVectorSparseScaler[separate:])
    print("\nkmeans label (sparse StandardScaler) => ")
    testApiVectorSparse = getTestVector(apiVectorSparseScaler, 2, len(malwareFamilies))
    # print("kmenas predict (sparse StandardScaler) => " + str(k_means.predict(testApiVectorSparse)) + "\n")
    printSeparate(k_means.labels_, k_means.predict(testApiVectorSparse), numberFromEachFamily, 2)
    # print("apiVectorSparseScaler[0] => " + str(apiVectorSparseScaler[0]))
    # print("apiVectorSparseScaler[1] => " + str(apiVectorSparseScaler[1]))
    # print("apiVectorSparseScaler[20] => " + str(apiVectorSparseScaler[20]))
    # print("apiVectorSparseScaler[21] => " + str(apiVectorSparseScaler[21]))
    # print("\ntestApiVectorSparse[0] => " + str(testApiVectorSparse[0]))
    # print("\ntestApiVectorSparse[1] => " + str(testApiVectorSparse[1]))
    # print("\ntestApiVectorSparse[2] => " + str(testApiVectorSparse[2]))
    # print("\ntestApiVectorSparse[3] => " + str(testApiVectorSparse[3]))

    k_means = cluster.KMeans(n_clusters=clusterNum, random_state=0, precompute_distances=True).fit(apiVectorOutlier[separate:])
    print("\nkmeans label (outllier RobustScaler) => ")
    testApiVector = getTestVector(apiVectorOutlier, 2, len(malwareFamilies))
    # print("kmenas predict (outlier RobustScaler) => " + str(k_means.predict(testApiVectorSparse)) + "\n")
    printSeparate(k_means.labels_, k_means.predict(testApiVector), numberFromEachFamily, 2)

    # print("apiVectorOutlier[0] => " + str(apiVectorOutlier[0]))
    # print("apiVectorOutlier[1] => " + str(apiVectorOutlier[1]))
    # print("apiVectorOutlier[20] => " + str(apiVectorOutlier[20]))
    # print("apiVectorOutlier[21] => " + str(apiVectorOutlier[21]))
    # print("\ntestApiVector[0] => " + str(testApiVector[0]))
    # print("\ntestApiVector[1] => " + str(testApiVector[1]))
    # print("\ntestApiVector[2] => " + str(testApiVector[2]))
    # print("\ntestApiVector[3] => " + str(testApiVector[3]))
    print("\n### API Statistics Evaluation ###")
    print("############# FINISH #############\n")
    #
    ##########
